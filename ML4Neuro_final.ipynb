{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "kutSvO_GipeZ",
      "metadata": {
        "id": "kutSvO_GipeZ"
      },
      "source": [
        "# Explainable graph network for Graph Convolutional Networks for affordable epilepsy detection with EEG with affordable devices in Africa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "iHCIwWJMEb5e",
      "metadata": {
        "id": "iHCIwWJMEb5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3e27bd9-864e-40f9-89c5-75ef9e846e3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install torch-geometric -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "-1tHX9vQ6sDE",
      "metadata": {
        "id": "-1tHX9vQ6sDE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "228a8c1b-fea9-42b7-afeb-73304ab7201f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install mne -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ZYxul5xoR5hm",
      "metadata": {
        "id": "ZYxul5xoR5hm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import gzip\n",
        "import numpy as np\n",
        "import os\n",
        "import mne\n",
        "import json\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.nn import Module, Linear , LayerNorm, BatchNorm1d, Dropout\n",
        "#from torch_geometric.nn import GATConv\n",
        "from torch_geometric.nn import GATv2Conv\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from torch_geometric.nn import global_mean_pool, global_add_pool\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch.optim import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "import networkx as nx\n",
        "from collections import Counter\n",
        "\n",
        "mne.set_log_level(\"ERROR\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AbJxO4Fiir7r",
      "metadata": {
        "id": "AbJxO4Fiir7r"
      },
      "source": [
        "### About Data\n",
        "\n",
        "Electroencephalography (EEG) data collected in people with epilepsy (N=163) and healthy controls (N=138) in two difficult-to-reach areas in rural Guinea-Bissau and Nigeria. Five minutes of fourteen channel resting-state EEG data were acquired with a portable, low-cost consumer-grade EEG recording headset. Experiments started either with two minutes of eyes closed or two minutes of eyes open which was randomized across participants. The order of the experiment is clarified in the meta-data files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "Rwy1CPS2OEXC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rwy1CPS2OEXC",
        "outputId": "f14fe9e6-9012-42c8-a9fd-5ba35450ba4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "HiyNJc6rQn9i",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiyNJc6rQn9i",
        "outputId": "781ec995-c4c1-495b-9cc4-d7a5004834e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/DATASET\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "200edfb0-6084-465a-96d4-0afb346ac6a2",
      "metadata": {
        "id": "200edfb0-6084-465a-96d4-0afb346ac6a2"
      },
      "outputs": [],
      "source": [
        "gb_df = pd.read_csv(\"metadata_guineabissau.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3d7091f9-103a-4581-b502-3d463d1e52f4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "3d7091f9-103a-4581-b502-3d463d1e52f4",
        "outputId": "d17e4442-6383-4b4d-99d2-ae5a703c2624"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   subject.id     Group              Eyes.condition  \\\n",
              "0           1  Epilepsy  closed-3min-then-open-2min   \n",
              "1           2   Control  open-3min-then-closed-2min   \n",
              "2           3  Epilepsy  closed-3min-then-open-2min   \n",
              "3           4  Epilepsy  closed-3min-then-open-2min   \n",
              "4           5   Control  closed-3min-then-open-2min   \n",
              "\n",
              "                                       Remarks  recordedPeriod  \\\n",
              "0                 by 45s reposition electrodes             301   \n",
              "1                                          NaN             309   \n",
              "2                                          NaN             309   \n",
              "3  Green lights not shown, but good EEG traces             299   \n",
              "4                                          NaN             302   \n",
              "\n",
              "         startTime  \n",
              "0  27/5/2020 14:33  \n",
              "1  26/5/2020 22:44  \n",
              "2  27/5/2020 14:26  \n",
              "3  27/5/2020 15:23  \n",
              "4  23/5/2020 19:09  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-19f02f6a-289a-4d07-95f4-e41d33ac3dce\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subject.id</th>\n",
              "      <th>Group</th>\n",
              "      <th>Eyes.condition</th>\n",
              "      <th>Remarks</th>\n",
              "      <th>recordedPeriod</th>\n",
              "      <th>startTime</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Epilepsy</td>\n",
              "      <td>closed-3min-then-open-2min</td>\n",
              "      <td>by 45s reposition electrodes</td>\n",
              "      <td>301</td>\n",
              "      <td>27/5/2020 14:33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Control</td>\n",
              "      <td>open-3min-then-closed-2min</td>\n",
              "      <td>NaN</td>\n",
              "      <td>309</td>\n",
              "      <td>26/5/2020 22:44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Epilepsy</td>\n",
              "      <td>closed-3min-then-open-2min</td>\n",
              "      <td>NaN</td>\n",
              "      <td>309</td>\n",
              "      <td>27/5/2020 14:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Epilepsy</td>\n",
              "      <td>closed-3min-then-open-2min</td>\n",
              "      <td>Green lights not shown, but good EEG traces</td>\n",
              "      <td>299</td>\n",
              "      <td>27/5/2020 15:23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Control</td>\n",
              "      <td>closed-3min-then-open-2min</td>\n",
              "      <td>NaN</td>\n",
              "      <td>302</td>\n",
              "      <td>23/5/2020 19:09</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-19f02f6a-289a-4d07-95f4-e41d33ac3dce')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-19f02f6a-289a-4d07-95f4-e41d33ac3dce button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-19f02f6a-289a-4d07-95f4-e41d33ac3dce');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9377a506-af45-4b5c-a8c6-05a3e044f684\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9377a506-af45-4b5c-a8c6-05a3e044f684')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9377a506-af45-4b5c-a8c6-05a3e044f684 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "gb_df",
              "summary": "{\n  \"name\": \"gb_df\",\n  \"rows\": 97,\n  \"fields\": [\n    {\n      \"column\": \"subject.id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28,\n        \"min\": 1,\n        \"max\": 97,\n        \"num_unique_values\": 97,\n        \"samples\": [\n          63,\n          41,\n          94\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Group\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Control\",\n          \"Epilepsy\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Eyes.condition\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"open-3min-then-closed-2min\",\n          \"closed-3min-then-open-2min\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Remarks\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 22,\n        \"samples\": [\n          \"by 45s reposition electrodes\",\n          \"OC yellow/orange signal\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recordedPeriod\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 27,\n        \"min\": 286,\n        \"max\": 478,\n        \"num_unique_values\": 33,\n        \"samples\": [\n          295,\n          425\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"startTime\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 97,\n        \"samples\": [\n          \"27/5/2020 14:52\",\n          \"27/5/2020 14:02\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "gb_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5e38ec88-82f6-422e-8486-f82d283b1d50",
      "metadata": {
        "id": "5e38ec88-82f6-422e-8486-f82d283b1d50"
      },
      "outputs": [],
      "source": [
        "ni_df = pd.read_csv(\"metadata_nigeria.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "134d0f2b-f164-4e86-8ec7-02683fe5b13e",
      "metadata": {
        "id": "134d0f2b-f164-4e86-8ec7-02683fe5b13e"
      },
      "outputs": [],
      "source": [
        "ni_df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fB3u0CIZNXaQ",
      "metadata": {
        "id": "fB3u0CIZNXaQ"
      },
      "source": [
        "## Datasets transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "hHgp6Dt1NUX7",
      "metadata": {
        "id": "hHgp6Dt1NUX7"
      },
      "outputs": [],
      "source": [
        "def file_exists(file_path):\n",
        "    return os.path.isfile(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rBpBeqrm2RCk",
      "metadata": {
        "id": "rBpBeqrm2RCk"
      },
      "source": [
        "### Nigeria"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7vJHNL2HNm6g",
      "metadata": {
        "id": "7vJHNL2HNm6g"
      },
      "source": [
        "Adding new columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "TeDNSsV83WZn",
      "metadata": {
        "id": "TeDNSsV83WZn"
      },
      "outputs": [],
      "source": [
        "ni_df[\"Eyes.condition\"] = ni_df[\"first_condition\"].apply(lambda x: \"open-3min-then-closed-2min\" if x == \"open\" else \"closed-3min-then-open-2min\")\n",
        "# ni_df[\"csv.file\"] = ni_df[\"csv.file\"].apply(lambda x: \"EEGs_Nigeria/\"+x)\n",
        "ni_df[\"csv.file\"] = ni_df[\"csv.file\"].apply(lambda x: \"EEGs_Nigeria/\"+x)\n",
        "ni_df[\"Group\"] = ni_df[\"Group\"].apply(lambda x: x.title())\n",
        "ni_df[\"subject.id\"] = ni_df[\"subject.id\"].apply(lambda x: \"NI-\" + str(x))\n",
        "ni_df[\"Country\"] = \"Nigeria\"\n",
        "ni_df['file_exists'] = ni_df['csv.file'].apply(lambda x: file_exists(x))\n",
        "ni_df = ni_df.drop(columns=['session.id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "_84Dn3KGrYHB",
      "metadata": {
        "id": "_84Dn3KGrYHB"
      },
      "outputs": [],
      "source": [
        "def count_rows(file):\n",
        "  data = pd.read_csv(file, compression='gzip',\n",
        "                   on_bad_lines='skip')\n",
        "  return len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ixocG3mUokZ8",
      "metadata": {
        "id": "ixocG3mUokZ8"
      },
      "outputs": [],
      "source": [
        "ni_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ojd81eBPeXxd",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "id": "ojd81eBPeXxd",
        "outputId": "6c137d8e-9b33-4890-df48-ab81cec1a20c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://docs.google.com/spreadsheets/d/1zjprOANMuVy6g_HYm4Tp-UU4VQCGZasmkqWQGCSiIqY/edit#gid=0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x79dd0e8bac90>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"600\"\n",
              "            src=\"https://docs.google.com/spreadsheets/d/1zjprOANMuVy6g_HYm4Tp-UU4VQCGZasmkqWQGCSiIqY/edit?rm=embedded#gid=0\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import sheets\n",
        "sheet = sheets.InteractiveSheet(df=ni_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MoZjTyMjNrCK",
      "metadata": {
        "id": "MoZjTyMjNrCK"
      },
      "source": [
        "Some data cleaning will be required"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-r0HSbCLJKhs",
      "metadata": {
        "id": "-r0HSbCLJKhs"
      },
      "outputs": [],
      "source": [
        "ni_df[ni_df[\"recordedPeriod\"]<200]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Nle4rOsCHQ7u",
      "metadata": {
        "id": "Nle4rOsCHQ7u"
      },
      "outputs": [],
      "source": [
        "(ni_df[ni_df[\"recordedPeriod\"]<200])[\"remarks\"].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i63b5YwPE8GT",
      "metadata": {
        "id": "i63b5YwPE8GT"
      },
      "source": [
        "Removing data where dataset are not found in the files. Removing as well data for subject 124 as sample is split in two and difficult to generalize with the rest of data. Removing data that is marked for removal in the remarks - either mistake was made during a recording, recording was too short or poor quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "j93sw9-KM4Uw",
      "metadata": {
        "id": "j93sw9-KM4Uw"
      },
      "outputs": [],
      "source": [
        "samples_to_remove = [\"NI-124\", \"NI-536\", \"NI-522\", \"NI-508\", \"NI-515\", \"NI-580\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "Svc_2r_WErsu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Svc_2r_WErsu",
        "outputId": "69702394-45b5-4ae2-ee5c-155423460a93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    subject.id  recordedPeriod        startTime first_condition  \\\n",
            "0         NI-6             270  26/9/2016 13:13            open   \n",
            "1         NI-9             271  26/9/2016 13:30          closed   \n",
            "2        NI-10             272  26/9/2016 13:36            open   \n",
            "3        NI-11             274  26/9/2016 13:42          closed   \n",
            "4        NI-12             271  26/9/2016 13:47            open   \n",
            "..         ...             ...              ...             ...   \n",
            "216     NI-634             268  1/10/2016 16:25          closed   \n",
            "217     NI-635             268  1/10/2016 16:31            open   \n",
            "218     NI-636             268  1/10/2016 16:36          closed   \n",
            "219     NI-637             268  1/10/2016 16:43            open   \n",
            "220     NI-638             273  1/10/2016 16:49          closed   \n",
            "\n",
            "                                               remarks     Group  \\\n",
            "0                                                  NaN   Control   \n",
            "1                                                  NaN   Control   \n",
            "2                                  eyes closed at 2:40   Control   \n",
            "3            no.11.1 failed  >>  11.2 is the right one   Control   \n",
            "4                                                  NaN   Control   \n",
            "..                                                 ...       ...   \n",
            "216                                                NaN  Epilepsy   \n",
            "217  Every day a convulsion + snapt het niet goed, ...  Epilepsy   \n",
            "218                                                NaN  Epilepsy   \n",
            "219                                                NaN  Epilepsy   \n",
            "220                             Medicijn Carbamazepin   Epilepsy   \n",
            "\n",
            "                             csv.file              Eyes.condition  Country  \\\n",
            "0      EEGs_Nigeria/signal-6-1.csv.gz  open-3min-then-closed-2min  Nigeria   \n",
            "1      EEGs_Nigeria/signal-9-1.csv.gz  closed-3min-then-open-2min  Nigeria   \n",
            "2     EEGs_Nigeria/signal-10-1.csv.gz  open-3min-then-closed-2min  Nigeria   \n",
            "3     EEGs_Nigeria/signal-11-2.csv.gz  closed-3min-then-open-2min  Nigeria   \n",
            "4     EEGs_Nigeria/signal-12-1.csv.gz  open-3min-then-closed-2min  Nigeria   \n",
            "..                                ...                         ...      ...   \n",
            "216  EEGs_Nigeria/signal-634-1.csv.gz  closed-3min-then-open-2min  Nigeria   \n",
            "217  EEGs_Nigeria/signal-635-1.csv.gz  open-3min-then-closed-2min  Nigeria   \n",
            "218  EEGs_Nigeria/signal-636-1.csv.gz  closed-3min-then-open-2min  Nigeria   \n",
            "219  EEGs_Nigeria/signal-637-1.csv.gz  open-3min-then-closed-2min  Nigeria   \n",
            "220  EEGs_Nigeria/signal-638-1.csv.gz  closed-3min-then-open-2min  Nigeria   \n",
            "\n",
            "     file_exists  \n",
            "0           True  \n",
            "1           True  \n",
            "2           True  \n",
            "3           True  \n",
            "4           True  \n",
            "..           ...  \n",
            "216         True  \n",
            "217         True  \n",
            "218         True  \n",
            "219         True  \n",
            "220         True  \n",
            "\n",
            "[214 rows x 10 columns]\n"
          ]
        }
      ],
      "source": [
        "ni_df = ni_df[(ni_df['file_exists']) & (~ni_df['subject.id'].isin(samples_to_remove))]\n",
        "ni_df = ni_df[(ni_df['file_exists']) & (~ni_df['subject.id'].isin(samples_to_remove))]\n",
        "\n",
        "# Check the result\n",
        "print(ni_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "hxvSOwP5rzI6",
      "metadata": {
        "id": "hxvSOwP5rzI6"
      },
      "outputs": [],
      "source": [
        "ni_df[\"file_rows_count\"] = ni_df[\"csv.file\"].apply(lambda x: count_rows(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DYhO77Wsm3eV",
      "metadata": {
        "id": "DYhO77Wsm3eV"
      },
      "outputs": [],
      "source": [
        "ni_df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y5UA83fAOHLd",
      "metadata": {
        "id": "y5UA83fAOHLd"
      },
      "source": [
        "### Guinea-Bissau"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XiIezqyNOdwG",
      "metadata": {
        "id": "XiIezqyNOdwG"
      },
      "source": [
        "Adding new columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SB0VC8VxOdwH",
      "metadata": {
        "id": "SB0VC8VxOdwH"
      },
      "outputs": [],
      "source": [
        "gb_df[\"first_condition\"] = gb_df['Eyes.condition'].str.split('-').str[0]\n",
        "gb_df[\"csv.file\"] = gb_df[\"subject.id\"].apply(lambda x: \"EEGs_Guinea-Bissau/signal-\" + str(x) + \".csv.gz\")\n",
        "gb_df[\"Group\"] = gb_df[\"Group\"].apply(lambda x: x.title())\n",
        "gb_df[\"subject.id\"] = gb_df[\"subject.id\"].apply(lambda x: \"GB-\" + str(x))\n",
        "gb_df[\"Country\"] = \"Guinea Bissau\"\n",
        "gb_df['file_exists'] = gb_df['csv.file'].apply(lambda x: file_exists(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Yi1cyKpn81Tc",
      "metadata": {
        "id": "Yi1cyKpn81Tc"
      },
      "outputs": [],
      "source": [
        "gb_df = gb_df[(gb_df['file_exists'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oNyG0Thw81Td",
      "metadata": {
        "id": "oNyG0Thw81Td"
      },
      "outputs": [],
      "source": [
        "gb_df[\"file_rows_count\"] = gb_df[\"csv.file\"].apply(lambda x: count_rows(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4TsGBIEc81Td",
      "metadata": {
        "id": "4TsGBIEc81Td"
      },
      "outputs": [],
      "source": [
        "gb_df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47y59ltIObkK",
      "metadata": {
        "id": "47y59ltIObkK"
      },
      "source": [
        "## Display data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d298774a-4c13-496c-8821-dc8e475591bf",
      "metadata": {
        "id": "d298774a-4c13-496c-8821-dc8e475591bf"
      },
      "outputs": [],
      "source": [
        "def plot_frequency_all(path):\n",
        "\n",
        "    data = pd.read_csv(path, compression='gzip',\n",
        "                       on_bad_lines='skip')\n",
        "\n",
        "    # Get the x-axis (first column) and the y-values (all other columns)\n",
        "    x = data.iloc[:, 0]\n",
        "    columns = data.columns[1:15]  # Skip the first column for the y-values\n",
        "\n",
        "    # Create subplots\n",
        "    fig, axes = plt.subplots(nrows=len(columns), ncols=1, figsize=(12, 20), sharex=True)\n",
        "\n",
        "    for i, col in enumerate(columns):\n",
        "        axes[i].plot(x, data[col], label=col)\n",
        "        axes[i].set_title(col)\n",
        "        axes[i].set_ylabel(\"Value\")\n",
        "        axes[i].grid(True)\n",
        "        axes[i].legend()\n",
        "\n",
        "    # Set common x-label\n",
        "    plt.xlabel(\"Index\")\n",
        "    # plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IeQD3ZU_gzup",
      "metadata": {
        "id": "IeQD3ZU_gzup"
      },
      "outputs": [],
      "source": [
        "def plot_frequency_closed(path, ni_df):\n",
        "    \"\"\"\n",
        "    Plot the frequency data from the specified .gz file, displaying only a subset\n",
        "    of rows based on the `first_condition` and `row_count` from the ni_df dataframe.\n",
        "    Purpose is cutting data to only the data that represent closed eyes state.\n",
        "\n",
        "    Parameters:\n",
        "        path (str): Path to the .gz file.\n",
        "        ni_df (pd.DataFrame): DataFrame containing metadata about the files, including\n",
        "                              `row_count` and `first_condition`.\n",
        "    \"\"\"\n",
        "    # Lookup row_count and first_condition in ni_df\n",
        "    row_info = ni_df[ni_df['csv.file'] == path]\n",
        "\n",
        "    if row_info.empty:\n",
        "        raise ValueError(f\"No matching entry found for file path: {path}\")\n",
        "\n",
        "    row_count = row_info['file_rows_count'].values[0]\n",
        "    first_condition = row_info['first_condition'].values[0]\n",
        "\n",
        "    # Calculate the number of rows to read (0.4 of the total rows)\n",
        "    rows_to_read = int(row_count * 0.4)\n",
        "\n",
        "    # Load the relevant portion of data\n",
        "    with gzip.open(path, 'rt') as f:\n",
        "        if first_condition == \"closed\":\n",
        "            # Read the first 0.4 of rows, including the header\n",
        "            data = pd.read_csv(f, nrows=rows_to_read + 1)  # +1 to include the header\n",
        "        elif first_condition == \"open\":\n",
        "            # Calculate how many rows to skip but include the header\n",
        "            skip_rows = row_count - rows_to_read\n",
        "            data = pd.read_csv(f, skiprows=range(1, skip_rows + 1))  # Skip initial rows but not the header\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid first_condition: {first_condition}. Must be 'open' or 'closed'.\")\n",
        "\n",
        "    # Get the x-axis (first column) and the y-values (all other columns)\n",
        "    x = data.iloc[:, 0]\n",
        "    columns = data.columns[1:15]  # Use the first 14 columns after the x-axis for plotting\n",
        "\n",
        "    # Create subplots\n",
        "    fig, axes = plt.subplots(nrows=len(columns), ncols=1, figsize=(12, 20), sharex=True)\n",
        "\n",
        "    for i, col in enumerate(columns):\n",
        "        axes[i].plot(x, data[col], label=col)\n",
        "        axes[i].set_title(col)\n",
        "        axes[i].set_ylabel(\"Value\")\n",
        "        axes[i].grid(True)\n",
        "        axes[i].legend()\n",
        "\n",
        "    # Set common x-label\n",
        "    plt.xlabel(\"Index\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0yPr5sG4f1nH",
      "metadata": {
        "id": "0yPr5sG4f1nH"
      },
      "source": [
        "Let's see some example data from samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6543580a-eda9-4c1a-ad96-5d8675faea08",
      "metadata": {
        "id": "6543580a-eda9-4c1a-ad96-5d8675faea08"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('EEGs_Nigeria/signal-10-1.csv.gz',    on_bad_lines='skip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b87f1866-c067-42de-b802-0f2248f5a5a6",
      "metadata": {
        "id": "b87f1866-c067-42de-b802-0f2248f5a5a6"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZgRpu6Fpnpae",
      "metadata": {
        "id": "ZgRpu6Fpnpae"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JHjzkw5ieT2b",
      "metadata": {
        "id": "JHjzkw5ieT2b"
      },
      "source": [
        "Let's take a look at the plots for some sample signal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f394e4b-8963-47d7-9176-6d7790127199",
      "metadata": {
        "id": "3f394e4b-8963-47d7-9176-6d7790127199"
      },
      "outputs": [],
      "source": [
        "plot_frequency_all(\"EEGs_Nigeria/signal-10-1.csv.gz\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gEALSCNFeNiJ",
      "metadata": {
        "id": "gEALSCNFeNiJ"
      },
      "source": [
        "Display frequencies plot only for the closed eyes state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ClmV240WhGLo",
      "metadata": {
        "id": "ClmV240WhGLo"
      },
      "outputs": [],
      "source": [
        "plot_frequency_closed(\"EEGs_Nigeria/signal-10-1.csv.gz\", ni_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "v_0oh0RNeg0W",
      "metadata": {
        "id": "v_0oh0RNeg0W"
      },
      "source": [
        "Same for Guinea-Bissau data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "4XjJAiOxgFSt",
      "metadata": {
        "id": "4XjJAiOxgFSt"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('EEGs_Guinea-Bissau/signal-1.csv.gz', compression='gzip',\n",
        "                   on_bad_lines='skip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SrygAZFGgFSt",
      "metadata": {
        "id": "SrygAZFGgFSt"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rdE_2-X0n2uq",
      "metadata": {
        "id": "rdE_2-X0n2uq"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FJWK5TNbgFSt",
      "metadata": {
        "id": "FJWK5TNbgFSt"
      },
      "outputs": [],
      "source": [
        "plot_frequency_all(\"EEGs_Guinea-Bissau/signal-10.csv.gz\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HymdbJyFrYvN",
      "metadata": {
        "id": "HymdbJyFrYvN"
      },
      "outputs": [],
      "source": [
        "plot_frequency_closed(\"EEGs_Guinea-Bissau/signal-10.csv.gz\", gb_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xxc7rNzIp_P-",
      "metadata": {
        "id": "xxc7rNzIp_P-"
      },
      "source": [
        "## Building Pipeline and model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WLTjDU5OaIre",
      "metadata": {
        "id": "WLTjDU5OaIre"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "LiQHkdJryNLE",
      "metadata": {
        "id": "LiQHkdJryNLE"
      },
      "source": [
        "### Load and process EEG data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D_vtKQzEkeRF",
      "metadata": {
        "id": "D_vtKQzEkeRF"
      },
      "source": [
        "In this step, we define a function to load EEG data stored in .gz files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "xVrKpH7jyMuQ",
      "metadata": {
        "id": "xVrKpH7jyMuQ"
      },
      "outputs": [],
      "source": [
        "def load_eeg_from_gz(file_path, row_count, first_condition):\n",
        "    \"\"\"\n",
        "    Load a specific portion of the data from a .gz file based on the condition.\n",
        "    Required to cut dataset only to the closed eyes data.\n",
        "    Parameters:\n",
        "        file_path (str): Path to the .gz file.\n",
        "        row_count (int): Total number of rows in the file.\n",
        "        first_condition (str): Condition to decide which portion of data to load (\"open\" or \"closed\").\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Subset of the EEG data.\n",
        "    \"\"\"\n",
        "    perc =0.5\n",
        "    # Calculate the number of rows to read (0.4 of the total rows)\n",
        "    rows_to_read = int(row_count * perc)\n",
        "\n",
        "    with gzip.open(file_path, 'rt') as f:\n",
        "      if first_condition == \"closed\":\n",
        "            # Read the first 0.perc of rows\n",
        "            eeg_data = pd.read_csv(f, nrows=rows_to_read)\n",
        "      elif first_condition == \"open\":\n",
        "            # Skip rows to get the last perc.4 of rows\n",
        "            skip_rows = row_count - rows_to_read\n",
        "            eeg_data = pd.read_csv(f, skiprows=range(1, skip_rows + 1))  # Skip the header and initial rows\n",
        "      else:\n",
        "            raise ValueError(f\"Invalid first_condition: {first_condition}. Must be 'open' or 'closed'.\")\n",
        "\n",
        "    return eeg_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4ojZxVBkh9x",
      "metadata": {
        "id": "f4ojZxVBkh9x"
      },
      "source": [
        "Next, we preprocess the raw EEG data. This involves:\n",
        "\n",
        "1. Selecting Relevant Columns: Extracting columns that represent EEG channels.\n",
        "2. Creating an `MNE RawArray` Object: Converting the DataFrame into an MNE object to take advantage of its built-in signal processing functions.\n",
        "3. Applying Bandpass Filtering: Filtering the signal to focus on relevant frequencies (1–30 Hz) and remove noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "HGD6P8WA7uJN",
      "metadata": {
        "id": "HGD6P8WA7uJN"
      },
      "outputs": [],
      "source": [
        "# @title Default title text\n",
        "def preprocess_eeg(raw_data, low_freq=1, high_freq=30, sfreq=128):\n",
        "    # Extract columns 1–14 (EEG data)\n",
        "    eeg_data = raw_data.iloc[:, 0:14]\n",
        "    ch_names = eeg_data.columns.tolist()  # Channel names (columns 1–14)\n",
        "    ch_types = [\"eeg\"] * len(ch_names)  # All channels as EEG\n",
        "\n",
        "    # Create MNE RawArray object\n",
        "    raw_array = mne.io.RawArray(\n",
        "        eeg_data.values.T,\n",
        "        mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
        "    )\n",
        "    raw_array .set_eeg_reference()\n",
        "\n",
        "    # Bandpass filtering (1–30 Hz)\n",
        "    raw_array.filter(low_freq, high_freq, fir_design='firwin')\n",
        "\n",
        "   #epochs=mne.make_fixed_length_epochs(data,duration=5,overlap=1)\n",
        "   # epochs=epochs.drop_bad()\n",
        "\n",
        "\n",
        "    #raw_array = normalize_amplitude(raw_array)\n",
        "    return raw_array\n",
        "\n",
        "\n",
        "def preprocess_eeg_with_epochs(raw_data, duration=5, overlap=1, sfreq=128):\n",
        "    \"\"\"\n",
        "    Preprocess EEG data with epoching and filtering.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    raw_data : pd.DataFrame\n",
        "        Raw EEG data\n",
        "    duration : float\n",
        "        Duration of each epoch in seconds\n",
        "    overlap : float\n",
        "        Overlap between epochs in seconds\n",
        "    sfreq : float\n",
        "        Sampling frequency\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    epochs : mne.Epochs\n",
        "        Preprocessed and epoched data\n",
        "    n_epochs : int\n",
        "        Number of valid epochs after dropping bad ones\n",
        "    \"\"\"\n",
        "    # Extract EEG data (columns 1-14)\n",
        "    eeg_data = raw_data.iloc[:, 1:15]    #it should be 1:15\n",
        "\n",
        "    ch_names = eeg_data.columns.tolist()\n",
        "    ch_types = [\"eeg\"] * len(ch_names)\n",
        "\n",
        "    # Create MNE RawArray\n",
        "    raw_array = mne.io.RawArray(\n",
        "        eeg_data.values.T,\n",
        "        mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
        "    )\n",
        "\n",
        "    # Bandpass filtering (1-30 Hz)\n",
        "    raw_array.filter(1, 45, fir_design='firwin')\n",
        "\n",
        "    #ICA motion correction\n",
        "    ica = mne.preprocessing.ICA(n_components=10, random_state=97, max_iter='auto')\n",
        "    ica.fit(raw_array)\n",
        "\n",
        "    # Then exclude bad components (example: components [0, 1] are bad)\n",
        "    ica.exclude = [0, 1]  # Replace with real bad components after inspecting\n",
        "    raw_array = ica.apply(raw_array)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Create epochs\n",
        "    epochs = mne.make_fixed_length_epochs(raw_array, duration=duration, overlap=overlap)\n",
        "\n",
        "    # Drop bad epochs\n",
        "    epochs = epochs.drop_bad()\n",
        "\n",
        "    return epochs, len(epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "OrmugKlUM71a",
      "metadata": {
        "id": "OrmugKlUM71a"
      },
      "outputs": [],
      "source": [
        "def expand_labels_for_epochs(original_labels, n_epochs_per_sample):\n",
        "    \"\"\"\n",
        "    Expand labels to match the number of epochs for each sample.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    original_labels : list or np.array\n",
        "        Original labels for each sample\n",
        "    n_epochs_per_sample : list\n",
        "        Number of valid epochs for each sample\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    expanded_labels : np.array\n",
        "        Labels expanded to match the number of epochs\n",
        "    \"\"\"\n",
        "    expanded_labels = []\n",
        "    for label, n_epochs in zip(original_labels, n_epochs_per_sample):\n",
        "        expanded_labels.extend([label] * n_epochs)\n",
        "    return np.array(expanded_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-_F2gue-b-V2",
      "metadata": {
        "id": "-_F2gue-b-V2"
      },
      "outputs": [],
      "source": [
        "def normalize_amplitude(raw_data, technique='min_max', axis=None, keepdims=True):\n",
        "    \"\"\"\n",
        "    Normalize data using various techniques with flexible axis handling.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    raw_data : MNE Raw object or numpy array\n",
        "        Input data to be normalized\n",
        "    technique : str\n",
        "        Normalization technique to use:\n",
        "        - 'min_max': scales data to [0, 1] range\n",
        "        - 'min_max_symmetric': scales data to [-1, 1] range\n",
        "        - 'mean_std': standardizes data by removing mean and scaling to unit variance\n",
        "        - 'mean_only': only subtracts mean (centered data)\n",
        "    axis : int or None\n",
        "        Axis along which to compute statistics. If None, compute globally.\n",
        "        axis=1 for per-channel normalization\n",
        "    keepdims : bool\n",
        "        Whether to keep dimensions when computing statistics\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    raw_data : MNE Raw object or numpy array\n",
        "        Normalized data in the same format as input\n",
        "    \"\"\"\n",
        "    # Get the data array\n",
        "    if hasattr(raw_data, 'get_data'):\n",
        "        raw_data_values = raw_data.get_data()\n",
        "    else:\n",
        "        raw_data_values = raw_data.copy()\n",
        "\n",
        "    # Compute statistics along specified axis\n",
        "    min_val = raw_data_values.min(axis=axis, keepdims=keepdims)\n",
        "    max_val = raw_data_values.max(axis=axis, keepdims=keepdims)\n",
        "    mean_val = raw_data_values.mean(axis=axis, keepdims=keepdims)\n",
        "    std_val = raw_data_values.std(axis=axis, keepdims=keepdims)\n",
        "\n",
        "    # Handle division by zero cases\n",
        "    eps = 1e-8\n",
        "\n",
        "    # Apply normalization based on selected technique\n",
        "    if technique == 'min_max':\n",
        "        denominator = (max_val - min_val)\n",
        "        denominator = np.where(denominator == 0, eps, denominator)\n",
        "        normalized_data = (raw_data_values - min_val) / denominator\n",
        "\n",
        "    elif technique == 'min_max_symmetric':\n",
        "        denominator = (max_val - min_val)\n",
        "        denominator = np.where(denominator == 0, eps, denominator)\n",
        "        normalized_data = 2 * ((raw_data_values - min_val) / denominator) - 1\n",
        "\n",
        "    elif technique == 'mean_std':\n",
        "        denominator = std_val\n",
        "        denominator = np.where(denominator == 0, eps, denominator)\n",
        "        normalized_data = (raw_data_values - mean_val) / denominator\n",
        "\n",
        "    elif technique == 'mean_only':\n",
        "        normalized_data = raw_data_values - mean_val\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown normalization technique: {technique}\")\n",
        "\n",
        "    # Return data in the same format as input\n",
        "    if hasattr(raw_data, 'get_data'):\n",
        "        raw_data._data = normalized_data\n",
        "        return raw_data\n",
        "    else:\n",
        "        return normalized_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "U3kjanns6Iij",
      "metadata": {
        "id": "U3kjanns6Iij"
      },
      "source": [
        "The next step is to calculate the **Phase-Locking Value (PLV)** matrix, which measures the functional connectivity between EEG channels. The PLV quantifies how consistent the phase difference between two signals is over time. This is useful for identifying synchronized brain activity between regions.\n",
        "\n",
        "1. `calculate_plv_matrix` Function:\n",
        "\n",
        "Computes a pairwise PLV matrix for all EEG channels.\n",
        "Each entry in the matrix represents the PLV value between two channels.\n",
        "\n",
        "2. `calculate_plv` Function:\n",
        "\n",
        "Calculates the PLV between two individual EEG signals by comparing their phase differences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "d7gTOyRzyYpK",
      "metadata": {
        "id": "d7gTOyRzyYpK"
      },
      "outputs": [],
      "source": [
        "def calculate_plv_matrix(eeg_data):\n",
        "    n_channels = len(eeg_data.info['ch_names'])\n",
        "    plv_matrix = np.zeros((n_channels, n_channels))\n",
        "    for i in range(n_channels):\n",
        "        for j in range(i + 1, n_channels):\n",
        "            signal1 = eeg_data.get_data(picks=i).flatten()\n",
        "            signal2 = eeg_data.get_data(picks=j).flatten()\n",
        "            plv_matrix[i, j] = calculate_plv(signal1, signal2)\n",
        "    return plv_matrix\n",
        "\n",
        "def calculate_plv(signal1, signal2):\n",
        "    phase1 = np.angle(signal1)\n",
        "    phase2 = np.angle(signal2)\n",
        "    return np.abs(np.mean(np.exp(1j * (phase1 - phase2))))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L8oP87N_lL8u",
      "metadata": {
        "id": "L8oP87N_lL8u"
      },
      "source": [
        "**Feature Computation for EEG Channels**\n",
        "\n",
        "To provide more detailed information about each EEG channel, we compute specific features:\n",
        "\n",
        "1. *Katz Fractal Dimension (Katz FD):*\n",
        "\n",
        "A measure of signal complexity, often used in EEG analysis to quantify how irregular the signal is.\n",
        "\n",
        "2. *Band Energy:*\n",
        "\n",
        "The energy of the EEG signal in specific frequency bands:\n",
        "* Delta (1–4 Hz): Associated with deep sleep and unconscious states.\n",
        "* Theta (4–8 Hz): Linked to relaxation and meditative states.\n",
        "* Alpha (8–13 Hz): Represents calm and focused mental states.\n",
        "* Beta (13–30 Hz): Associated with active thinking and problem-solving.\n",
        "\n",
        "The function calculates these features for each EEG channel and returns them as a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "jDJv-AAOSKXX",
      "metadata": {
        "id": "jDJv-AAOSKXX"
      },
      "outputs": [],
      "source": [
        "def compute_features(eeg_data):\n",
        "    \"\"\"\n",
        "    Compute features (Katz FD, band energy) for each EEG channel.\n",
        "    :param eeg_data: MNE RawArray object with EEG channels.\n",
        "    :return: Dictionary with features for each channel.\n",
        "    \"\"\"\n",
        "    features = {}\n",
        "    sfreq = eeg_data.info['sfreq']\n",
        "\n",
        "    for ch_name in eeg_data.info['ch_names']:\n",
        "        signal = eeg_data.get_data(picks=ch_name).flatten()\n",
        "\n",
        "        # Katz FD\n",
        "        L = np.sum(np.sqrt(np.diff(signal)**2 + 1))\n",
        "        d = np.max(signal) - np.min(signal)\n",
        "        katz_fd = np.log10(L) / np.log10(d)\n",
        "\n",
        "        # Band energy (delta, theta, alpha, beta)\n",
        "        freqs, psd = mne.time_frequency.psd_array_welch(signal, sfreq=sfreq, n_fft=256)\n",
        "        delta_energy = np.sum(psd[(freqs >= 1) & (freqs < 4)])\n",
        "        theta_energy = np.sum(psd[(freqs >= 4) & (freqs < 8)])\n",
        "        alpha_energy = np.sum(psd[(freqs >= 8) & (freqs < 13)])\n",
        "        beta_energy = np.sum(psd[(freqs >= 13) & (freqs < 30)])\n",
        "\n",
        "        features[ch_name] = [katz_fd, delta_energy, theta_energy, alpha_energy, beta_energy]\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vyuKo5H4ls5W",
      "metadata": {
        "id": "vyuKo5H4ls5W"
      },
      "source": [
        "This step converts the **Phase-Locking Value (PLV)** matrix into a graph where:\n",
        "\n",
        "* **Nodes** represent EEG channels (e.g., T3, T4).\n",
        "* **Edges connect** channels based on their PLV values, indicating functional connectivity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "u3Tkxl2eSLCM",
      "metadata": {
        "id": "u3Tkxl2eSLCM"
      },
      "outputs": [],
      "source": [
        "def create_graph_from_plv(plv_matrix, channel_names):\n",
        "    \"\"\"\n",
        "    Create a graph from a PLV matrix using original channel names as node labels.\n",
        "    \"\"\"\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Add nodes with channel names as labels\n",
        "    for i, name in enumerate(channel_names):\n",
        "        G.add_node(name)  # Use the original channel name instead of numeric ID\n",
        "\n",
        "    # Add edges with weights\n",
        "    for i in range(len(plv_matrix)):\n",
        "        for j in range(i + 1, len(plv_matrix)):\n",
        "            if plv_matrix[i, j] > 0:  # Include edges with positive weights\n",
        "                G.add_edge(channel_names[i], channel_names[j], weight=plv_matrix[i, j])\n",
        "\n",
        "    return G"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qMnNq5wil3Qt",
      "metadata": {
        "id": "qMnNq5wil3Qt"
      },
      "source": [
        "This step automates the pipeline for multiple EEG recordings. For each file:\n",
        "\n",
        "1. **Load the EEG Data:** Read the .gz file containing the raw EEG data.\n",
        "2. **Preprocess the Signal:** Apply bandpass filtering to isolate relevant frequencies.\n",
        "3. **Calculate the PLV Matrix:** Compute pairwise functional connectivity between channels.\n",
        "4. **Create and Save the Graph:** Convert the PLV matrix into a graph and save it as a .gml file.\n",
        "5. **Compute and Save Node Features:** Extract features like Katz FD and band energy for each channel and save them in a JSON file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "1iY3HJsWylUi",
      "metadata": {
        "id": "1iY3HJsWylUi"
      },
      "outputs": [],
      "source": [
        "def process_all_files(metadata, output_dir, feature_output_dir):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    os.makedirs(feature_output_dir, exist_ok=True)\n",
        "\n",
        "    for _, row in metadata.iterrows():\n",
        "        # Load EEG data from .gz file\n",
        "        eeg_data = load_eeg_from_gz(row['csv.file'], row['file_rows_count'], row['first_condition'])\n",
        "\n",
        "        # Filtering and processing the signal\n",
        "        raw_array = preprocess_eeg(eeg_data.iloc[:, 1:15])\n",
        "\n",
        "        # Calculate PLV\n",
        "        plv_matrix = calculate_plv_matrix(raw_array)\n",
        "\n",
        "        # Create the graph\n",
        "        graph = create_graph_from_plv(plv_matrix, raw_array.info['ch_names'])\n",
        "\n",
        "        # Save the graph\n",
        "        # Convert edge weights to float (if they're np.float64)\n",
        "        for u, v, data in graph.edges(data=True):\n",
        "           if 'weight' in data:\n",
        "             data['weight'] = float(data['weight'])\n",
        "        nx.write_gml(graph, os.path.join(output_dir, f\"{row['subject.id']}_graph.gml\"))\n",
        "\n",
        "        # Compute features and save separately\n",
        "        features = compute_features(raw_array)\n",
        "        feature_file = os.path.join(feature_output_dir, f\"{row['subject.id']}_features.json\")\n",
        "        with open(feature_file, 'w') as f:\n",
        "            json.dump(features, f)\n",
        "\n",
        "\n",
        "def process_all_files_with_epochs(metadata, output_dir, feature_output_dir, duration=5, overlap=1):\n",
        "    \"\"\"\n",
        "    Process all EEG files with epoching and save results.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    metadata : pd.DataFrame\n",
        "        Metadata containing file information\n",
        "    output_dir : str\n",
        "        Directory to save graph files\n",
        "    feature_output_dir : str\n",
        "        Directory to save feature files\n",
        "    duration : float\n",
        "        Duration of each epoch in seconds\n",
        "    overlap : float\n",
        "        Overlap between epochs in seconds\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    n_epochs_per_sample : list\n",
        "        Number of valid epochs for each sample\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    os.makedirs(feature_output_dir, exist_ok=True)\n",
        "\n",
        "    n_epochs_per_sample = []\n",
        "\n",
        "    for idx, row in metadata.iterrows():\n",
        "        # Load EEG data\n",
        "        eeg_data = load_eeg_from_gz(row['csv.file'], row['file_rows_count'], row['first_condition'])\n",
        "\n",
        "        # Preprocess and create epochs\n",
        "        epochs, n_epochs = preprocess_eeg_with_epochs(eeg_data, duration=duration, overlap=overlap)\n",
        "        n_epochs_per_sample.append(n_epochs)\n",
        "\n",
        "        # Process each epoch\n",
        "        for epoch_idx in range(n_epochs):\n",
        "            epoch_data = epochs[epoch_idx]\n",
        "\n",
        "            # Calculate PLV for this epoch\n",
        "            plv_matrix = calculate_plv_matrix(epoch_data)\n",
        "\n",
        "            # Create graph for this epoch\n",
        "            graph = create_graph_from_plv(plv_matrix, epoch_data.info['ch_names'])\n",
        "\n",
        "            # Save graph with epoch index in filename\n",
        "            # Convert edge weights to float (if they're np.float64)\n",
        "            for u, v, data in graph.edges(data=True):\n",
        "              if 'weight' in data:\n",
        "                data['weight'] = float(data['weight'])\n",
        "\n",
        "            nx.write_gml(graph, os.path.join(output_dir, f\"{row['subject.id']}_epoch{epoch_idx}_graph.gml\"))\n",
        "\n",
        "            # Compute and save features for this epoch\n",
        "            features = compute_features(epoch_data)\n",
        "            feature_file = os.path.join(feature_output_dir, f\"{row['subject.id']}_epoch{epoch_idx}_features.json\")\n",
        "            with open(feature_file, 'w') as f:\n",
        "                json.dump(features, f)\n",
        "\n",
        "    return n_epochs_per_sample"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SdX1dnkQA3Ip",
      "metadata": {
        "id": "SdX1dnkQA3Ip"
      },
      "source": [
        "### Build GAT model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecMCY-HNnNy2",
      "metadata": {
        "id": "ecMCY-HNnNy2"
      },
      "source": [
        "This step combines the graphs saved in `.gml` files with the node features saved in `.json` files. Each graph is converted to a PyTorch Geometric `Data` object, which includes:\n",
        "\n",
        "* Node Features: Features like Katz FD and band energy.\n",
        "* Edge Index and Attributes: Connectivity and weights from the PLV matrix.\n",
        "* Labels: Encoded class labels (Control or Epilepsy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "hG9Y2XsHEYOd",
      "metadata": {
        "id": "hG9Y2XsHEYOd"
      },
      "outputs": [],
      "source": [
        "def load_graphs_and_labels_with_features(gml_dir, feature_dir, metadata):\n",
        "    \"\"\"\n",
        "    Load .gml graphs, enrich with features, and convert to PyTorch Geometric format.\n",
        "    \"\"\"\n",
        "    graphs = []\n",
        "    labels = []\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    metadata['encoded_group'] = label_encoder.fit_transform(metadata['Group'])\n",
        "\n",
        "    for _, row in metadata.iterrows():\n",
        "        gml_file = os.path.join(gml_dir, f\"{row['subject.id']}_graph.gml\")\n",
        "        feature_file = os.path.join(feature_dir, f\"{row['subject.id']}_features.json\")\n",
        "\n",
        "        if os.path.exists(gml_file) and os.path.exists(feature_file):\n",
        "            # Load graph\n",
        "            graph = nx.read_gml(gml_file)\n",
        "\n",
        "            # Load features\n",
        "            with open(feature_file, 'r') as f:\n",
        "                features = json.load(f)\n",
        "\n",
        "            # Map node labels to numeric indices\n",
        "            node_mapping = {node: idx for idx, node in enumerate(graph.nodes)}\n",
        "\n",
        "            # Convert edges to numeric indices\n",
        "            edge_index = torch.tensor([[node_mapping[u], node_mapping[v]] for u, v in graph.edges], dtype=torch.long).t().contiguous()\n",
        "\n",
        "            # Convert node features to tensor\n",
        "            node_features = torch.tensor([features[node] for node in graph.nodes], dtype=torch.float32)\n",
        "\n",
        "            # Get edge attributes (e.g., weights)\n",
        "            edge_attr = torch.tensor([data['weight'] for _, _, data in graph.edges(data=True)], dtype=torch.float32)\n",
        "\n",
        "            # Create PyTorch Geometric data object\n",
        "            data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr, num_nodes=len(graph.nodes), y=torch.tensor([row['encoded_group']]))\n",
        "\n",
        "            graphs.append(data)\n",
        "            labels.append(row['encoded_group'])\n",
        "\n",
        "    # Convert labels to tensor\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "    return graphs, labels\n",
        "\n",
        "def load_graphs_and_labels_with_epochs(gml_dir, feature_dir, metadata, n_epochs_per_sample):\n",
        "    \"\"\"\n",
        "    Load epoched graphs and expand labels accordingly.\n",
        "    \"\"\"\n",
        "    graphs = []\n",
        "\n",
        "    # Encode labels first\n",
        "    label_encoder = LabelEncoder()\n",
        "    metadata['encoded_group'] = label_encoder.fit_transform(metadata['Group'])\n",
        "\n",
        "    # Get original labels and expand them based on epochs\n",
        "    original_labels = metadata['encoded_group'].values\n",
        "    expanded_labels = expand_labels_for_epochs(original_labels, n_epochs_per_sample)\n",
        "\n",
        "    # Load each epoch's graph\n",
        "    current_epoch_idx = 0\n",
        "#    for idx, row in metadata.iterrows():\n",
        "#        n_epochs = n_epochs_per_sample[idx]\n",
        "\n",
        "    for i, (_, row) in enumerate(metadata.iterrows()):\n",
        "        n_epochs = n_epochs_per_sample[i]\n",
        "\n",
        "        for epoch_idx in range(n_epochs):\n",
        "            gml_file = os.path.join(gml_dir, f\"{row['subject.id']}_epoch{epoch_idx}_graph.gml\")\n",
        "            feature_file = os.path.join(feature_dir, f\"{row['subject.id']}_epoch{epoch_idx}_features.json\")\n",
        "\n",
        "            if os.path.exists(gml_file) and os.path.exists(feature_file):\n",
        "                # Load graph\n",
        "                graph = nx.read_gml(gml_file)\n",
        "\n",
        "                # Load features\n",
        "                with open(feature_file, 'r') as f:\n",
        "                    features = json.load(f)\n",
        "\n",
        "                # Create PyTorch Geometric data object\n",
        "                node_mapping = {node: idx for idx, node in enumerate(graph.nodes)}\n",
        "                edge_index = torch.tensor([[node_mapping[u], node_mapping[v]] for u, v in graph.edges], dtype=torch.long).t().contiguous()\n",
        "                node_features = torch.tensor([features[node] for node in graph.nodes], dtype=torch.float32)\n",
        "                edge_attr = torch.tensor([data['weight'] for _, _, data in graph.edges(data=True)], dtype=torch.float32)\n",
        "\n",
        "                data = Data(\n",
        "                    x=node_features,\n",
        "                    edge_index=edge_index,\n",
        "                    edge_attr=edge_attr,\n",
        "                    num_nodes=len(graph.nodes),\n",
        "                    y=torch.tensor([expanded_labels[current_epoch_idx]])\n",
        "                )\n",
        "\n",
        "                graphs.append(data)\n",
        "                current_epoch_idx += 1\n",
        "\n",
        "    # Convert expanded labels to tensor\n",
        "    labels = torch.tensor(expanded_labels, dtype=torch.long)\n",
        "    return graphs, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QlCs7u0snhXE",
      "metadata": {
        "id": "QlCs7u0snhXE"
      },
      "source": [
        "Defining the GAT Model used for classification:\n",
        "\n",
        "1. Applying Graph Attention Layers (`GATConv`): These layers compute node embeddings using attention mechanisms that weigh the importance of each node's neighbors.\n",
        "2. Combining Layers: The first GAT layer outputs intermediate embeddings, and the second outputs class probabilities for each graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "WoRvz7_bEmKt",
      "metadata": {
        "id": "WoRvz7_bEmKt"
      },
      "outputs": [],
      "source": [
        "class GATModel(Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, edge_dim=0,  heads=1,  dropout=0.3):\n",
        "        \"\"\"\n",
        "        Initialize the GAT model.\n",
        "        Args:\n",
        "            in_channels (int): Number of input node features.\n",
        "            hidden_channels (int): Number of hidden channels in the first GAT layer.\n",
        "            out_channels (int): Number of output classes.\n",
        "            heads (int): Number of attention heads in the first GAT layer (default=1).\n",
        "\n",
        "        super(GATModel, self).__init__()\n",
        "        #self.gat1 = GATConv(in_channels, hidden_channels, heads=heads, concat=True)  # First attention layer\n",
        "        #self.gat2 = GATConv(hidden_channels * heads, out_channels, heads=1, concat=False)  # Second layer\n",
        "        self.gat1 = GATv2Conv(in_channels, hidden_channels, heads=heads, concat=True)  # First attention layer\n",
        "        self.gat2 = GATv2Conv(hidden_channels * heads, out_channels, heads=1, concat=False)  # Second layer\n",
        "        \"\"\"\n",
        "\n",
        "        super(GATModel, self).__init__()\n",
        "        self.gat1 = GATv2Conv(in_channels, hidden_channels, heads=heads, concat=True, edge_dim=edge_dim)\n",
        "        self.norm1 = LayerNorm(hidden_channels * heads)\n",
        "\n",
        "        self.gat2 = GATv2Conv(hidden_channels * heads, hidden_channels, heads=1, concat=False, edge_dim=edge_dim)\n",
        "        self.norm2 = LayerNorm(hidden_channels)\n",
        "\n",
        "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
        "        self.bn1 = BatchNorm1d(hidden_channels)\n",
        "\n",
        "        self.lin2 = Linear(hidden_channels, out_channels)\n",
        "\n",
        "        #self.dropout = Dropout(p=dropout)  # 👈 this was missing\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, batch):\n",
        "    #def forward(self, x, edge_index, edge_attr):\n",
        "        \"\"\"\n",
        "        Forward pass of the GAT model.\n",
        "        Args:\n",
        "            x (torch.Tensor): Node features.\n",
        "            edge_index (torch.Tensor): Edge indices.\n",
        "            edge_attr (torch.Tensor): Edge attributes (weights).\n",
        "        Returns:\n",
        "            torch.Tensor: Log-softmax probabilities for each class.\n",
        "        \"\"\"\n",
        "        '''\n",
        "        #x = self.gat1(x, edge_index, edge_attr)  # First GAT layer\n",
        "        x = self.gat1(x, edge_index )  # First GAT layer\n",
        "\n",
        "        x = F.elu(x)  # Apply activation\n",
        "        #x = self.gat2(x, edge_index, edge_attr)  # Second GAT layer\n",
        "        x = self.gat2(x, edge_index)  # Second GAT layer\n",
        "        '''\n",
        "        # First GAT + activation + norm\n",
        "        x = self.gat1(x, edge_index, edge_attr)\n",
        "        x = F.elu(x)\n",
        "        x = self.norm1(x)\n",
        "        #x = self.dropout(x)\n",
        "\n",
        "        # Second GAT + norm + activation\n",
        "        x = self.gat2(x, edge_index, edge_attr)\n",
        "        x = self.norm2(x)\n",
        "        x = F.elu(x)\n",
        "        #x = self.dropout(x)\n",
        "\n",
        "        # Global mean pooling to graph-level representation\n",
        "        #x = global_mean_pool(x, batch)\n",
        "        x = global_add_pool(x, batch)\n",
        "\n",
        "        # MLP: Linear -> Activation -> BN -> Linear\n",
        "        x = self.lin1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        #x = self.dropout(x)\n",
        "\n",
        "        x = self.lin2(x)\n",
        "        return F.log_softmax(x, dim=1)  # Output class probabilities\n",
        "        #return x\n",
        "        #return torch.sigmoid(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WVvGO7y6qFKq",
      "metadata": {
        "id": "WVvGO7y6qFKq"
      },
      "source": [
        "**Using Grad-CAM for Node Importance in GAT**\n",
        "\n",
        "Grad-CAM is used to identify the importance of nodes in the graph by analyzing gradients from a specific layer (gat1). The class GradCamGAT computes node importance based on these gradients.\n",
        "\n",
        "1. The forward method passes data through the target layer and captures gradients via a hook. The global mean pooling aggregates node-level outputs to graph-level predictions.\n",
        "2. The backward method computes gradients with respect to the graph output.\n",
        "3. The get_node_importance method calculates node importance by averaging gradients across features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "YSoIJ5SnZmfd",
      "metadata": {
        "id": "YSoIJ5SnZmfd"
      },
      "outputs": [],
      "source": [
        "class GradCamGAT:\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "\n",
        "        # Register forward hook on the target layer\n",
        "        self.target_layer.register_forward_hook(self.save_activation)\n",
        "\n",
        "    def save_gradient(self, grad):\n",
        "        # Save gradients during backward pass\n",
        "        self.gradients = grad\n",
        "\n",
        "    def save_activation(self, module, input, output):\n",
        "        # Save activations during forward pass\n",
        "        self.activations = output\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, batch):\n",
        "        # Forward pass through the target layer\n",
        "        node_output = self.target_layer(x, edge_index, edge_attr)\n",
        "        node_output.register_hook(self.save_gradient)  # Register gradient hook\n",
        "        graph_output = global_mean_pool(node_output, batch)  # Global pooling\n",
        "        return graph_output, node_output\n",
        "\n",
        "    def backward(self, output, class_idx=None):\n",
        "        # Backward pass to compute gradients for a specific class\n",
        "        if class_idx is None:\n",
        "            class_idx = output.argmax(dim=1)  # Use the predicted class if no class is specified\n",
        "        one_hot = torch.zeros_like(output)\n",
        "        one_hot[0, class_idx] = 1  # Create one-hot vector for the target class\n",
        "        output.backward(gradient=one_hot, retain_graph=True)\n",
        "\n",
        "    def get_node_importance(self):\n",
        "        # Compute node importance using gradients and activations\n",
        "        if self.gradients is None or self.activations is None:\n",
        "            raise ValueError(\"Gradients or activations have not been computed. Did you call forward and backward?\")\n",
        "        gradients = self.gradients.mean(dim=1, keepdim=True)  # Average gradients over nodes\n",
        "        activations = self.activations  # Use saved activations\n",
        "        node_importance = (gradients * activations).sum(dim=1)  # Grad-CAM formula\n",
        "        return node_importance.detach()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fs1JMa3yey1G",
      "metadata": {
        "id": "fs1JMa3yey1G"
      },
      "source": [
        "# Nigerian dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GWqeSoDjsR34",
      "metadata": {
        "id": "GWqeSoDjsR34"
      },
      "source": [
        "We will now proceed to use the above pipeline for Nigerian dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HzwBeEZEk_KL",
      "metadata": {
        "id": "HzwBeEZEk_KL"
      },
      "source": [
        "### GAT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "4Nt415yqftnJ",
      "metadata": {
        "id": "4Nt415yqftnJ"
      },
      "outputs": [],
      "source": [
        "# Directory containing .gml files\n",
        "gml_dir = \"output4/graphs_nigeria\"\n",
        "feature_dir = \"output4/features_nigeria\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VKJq39sSAyVs",
      "metadata": {
        "id": "VKJq39sSAyVs"
      },
      "source": [
        "Processing all EEG files, create graphs and feature files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "RbREOZm56VXl",
      "metadata": {
        "id": "RbREOZm56VXl"
      },
      "outputs": [],
      "source": [
        "#process_all_files(ni_df, \"output4/graphs_nigeria\", \"output4/features_nigeria\")\n",
        "\n",
        "n_epochs_per_sample = process_all_files_with_epochs(\n",
        "    ni_df,\n",
        "    \"output4/graphs_nigeria_epoched\",\n",
        "    \"output4/features_nigeria_epoched\",\n",
        "    duration=5,\n",
        "    overlap=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8Qf3zeTy9JYF",
      "metadata": {
        "id": "8Qf3zeTy9JYF"
      },
      "source": [
        "Loading a sample graph from the generated .gml file and inspect its structure:\n",
        "\n",
        "* **Nodes:** Represent EEG channels.\n",
        "* **Edges:** Represent functional connectivity (PLV) between channels.\n",
        "* **Edge Weights:** Represent the strength of connectivity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dqDyc7Wy6-iz",
      "metadata": {
        "id": "dqDyc7Wy6-iz"
      },
      "outputs": [],
      "source": [
        "# Path to a sample .gml file\n",
        "gml_file = \"output4/graphs_nigeria_epoched/NI-638_epoch20_graph.gml\"\n",
        "\n",
        "# Load the graph from the .gml file\n",
        "graph = nx.read_gml(gml_file)\n",
        "\n",
        "# Print basic information about the graph\n",
        "print(\"Number of nodes:\", graph.number_of_nodes())\n",
        "print(\"Number of edges:\", graph.number_of_edges())\n",
        "\n",
        "# Display a few edges with their weights\n",
        "print(\"Sample edges with weights:\")\n",
        "for u, v, weight in graph.edges(data='weight'):\n",
        "    print(f\"({u}, {v}) -> weight: {weight}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kpjXAsiqmtKA",
      "metadata": {
        "id": "kpjXAsiqmtKA"
      },
      "source": [
        "Visualizing the graph to better understand the connectivity:\n",
        "\n",
        "* Nodes are labeled with their channel names.\n",
        "* Edge weights are displayed, showing the strength of the connections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5g1FyAK69P64",
      "metadata": {
        "id": "5g1FyAK69P64"
      },
      "outputs": [],
      "source": [
        "# Load the graph\n",
        "graph = nx.read_gml(gml_file)\n",
        "\n",
        "# Draw the graph\n",
        "plt.figure(figsize=(10, 8))\n",
        "pos = nx.spring_layout(graph)  # Automatic layout for nodes\n",
        "nx.draw(\n",
        "    graph,\n",
        "    pos,\n",
        "    with_labels=True,\n",
        "    node_size=700,\n",
        "    node_color=\"lightblue\",\n",
        "    font_size=10,\n",
        "    font_color=\"black\",\n",
        ")\n",
        "# Add edge labels showing weights\n",
        "nx.draw_networkx_edge_labels(\n",
        "    graph, pos, edge_labels={(u, v): f\"{d['weight']:.2f}\" for u, v, d in graph.edges(data=True)}\n",
        ")\n",
        "plt.title(\"Visualization of EEG Graph (PLV)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5vht8tJdsp6o",
      "metadata": {
        "id": "5vht8tJdsp6o"
      },
      "source": [
        "Edges that connect different nodes (representing parts of brain) hold weights that imply how closely correlated are signals for these parts of brain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "JmZcYav8EqyC",
      "metadata": {
        "id": "JmZcYav8EqyC"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "#graphs, labels = load_graphs_and_labels_with_features(gml_dir, feature_dir, ni_df)\n",
        "# Load the epoched data with properly expanded labels\n",
        "graphs, labels = load_graphs_and_labels_with_epochs(\n",
        "    \"output4/graphs_nigeria_epoched\",\n",
        "    \"output4/features_nigeria_epoched\",\n",
        "    ni_df,\n",
        "    n_epochs_per_sample\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FIJKrlR4oQ68",
      "metadata": {
        "id": "FIJKrlR4oQ68"
      },
      "source": [
        "To ensure that both classes (Control and Epilepsy) are proportionally represented in the train and test datasets, we use a stratified split. This ensures the class balance is maintained in both subsets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "CAIJ499gXJz2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAIJ499gXJz2",
        "outputId": "7876cbc6-bddc-4fe8-b368-5a5efb8a0712"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train class distribution: Counter({np.int64(1): 2499, np.int64(0): 1908})\n",
            "Test class distribution: Counter({np.int64(1): 625, np.int64(0): 477})\n"
          ]
        }
      ],
      "source": [
        "# Stratified split to ensure both classes are represented in train and test sets\n",
        "train_graphs, test_graphs, train_labels, test_labels = train_test_split(\n",
        "    graphs, labels, test_size=0.2, stratify=labels, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Train class distribution:\", Counter(train_labels.numpy()))\n",
        "print(\"Test class distribution:\", Counter(test_labels.numpy()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XQ-3hRuyoYWE",
      "metadata": {
        "id": "XQ-3hRuyoYWE"
      },
      "source": [
        "Setting up the DataLoader and Model Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "zVZz5msFXNhp",
      "metadata": {
        "id": "zVZz5msFXNhp"
      },
      "outputs": [],
      "source": [
        "# Create DataLoader\n",
        "train_loader = DataLoader(train_graphs, batch_size=16, shuffle=True)\n",
        "\n",
        "# Initialize model\n",
        "#model = GATModel(in_channels=5, hidden_channels=8, out_channels=2)  # Binary classification\n",
        "model = GATModel(in_channels=5, hidden_channels=16, out_channels=2, heads=6)\n",
        "#optimizer = Adam(model.parameters(), lr=0.01)\n",
        "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=5e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ucLZ2EYKon3i",
      "metadata": {
        "id": "ucLZ2EYKon3i"
      },
      "source": [
        "The model is trained for 50 epochs. Here's how it works:\n",
        "\n",
        "1. Training Mode: The model is set to `train()` mode.\n",
        "2. Forward Pass:\n",
        "> - Compute node-level embeddings using the GAT layers.\n",
        "> - Aggregate node embeddings to graph-level predictions using `global_mean_pool`.\n",
        "3. Loss Calculation: Compute the negative log-likelihood loss between predicted and true labels.\n",
        "4. Backward Pass and Optimization: Update model weights using backpropagation and the Adam optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "kcNAZjE8WRsl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcNAZjE8WRsl",
        "outputId": "e225cec0-10de-42c0-aff6-70d0c72d656f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 187.41865992546082\n",
            "Epoch 2, Loss: 181.57717514038086\n",
            "Epoch 3, Loss: 180.6184875369072\n",
            "Epoch 4, Loss: 179.17058417201042\n",
            "Epoch 5, Loss: 178.18876558542252\n",
            "Epoch 6, Loss: 177.03513932228088\n",
            "Epoch 7, Loss: 176.59065699577332\n",
            "Epoch 8, Loss: 175.9703362584114\n",
            "Epoch 9, Loss: 173.13529780507088\n",
            "Epoch 10, Loss: 173.2721626162529\n",
            "Epoch 11, Loss: 173.35892432928085\n",
            "Epoch 12, Loss: 174.13810521364212\n",
            "Epoch 13, Loss: 173.94898653030396\n",
            "Epoch 14, Loss: 171.63072794675827\n",
            "Epoch 15, Loss: 169.90258753299713\n",
            "Epoch 16, Loss: 169.1829316318035\n",
            "Epoch 17, Loss: 169.001834243536\n",
            "Epoch 18, Loss: 169.54145017266273\n",
            "Epoch 19, Loss: 168.21198716759682\n",
            "Epoch 20, Loss: 167.36278399825096\n",
            "Epoch 21, Loss: 167.32471150159836\n",
            "Epoch 22, Loss: 167.36064705252647\n",
            "Epoch 23, Loss: 165.6844358742237\n",
            "Epoch 24, Loss: 166.44317176938057\n",
            "Epoch 25, Loss: 165.68602538108826\n",
            "Epoch 26, Loss: 166.94957980513573\n",
            "Epoch 27, Loss: 167.15816363692284\n",
            "Epoch 28, Loss: 166.29526168107986\n",
            "Epoch 29, Loss: 166.69348648190498\n",
            "Epoch 30, Loss: 164.67374095320702\n",
            "Epoch 31, Loss: 163.75731909275055\n",
            "Epoch 32, Loss: 163.95554810762405\n",
            "Epoch 33, Loss: 161.41214987635612\n",
            "Epoch 34, Loss: 162.86269435286522\n",
            "Epoch 35, Loss: 163.81878125667572\n",
            "Epoch 36, Loss: 161.743039727211\n",
            "Epoch 37, Loss: 163.38032907247543\n",
            "Epoch 38, Loss: 160.81190100312233\n",
            "Epoch 39, Loss: 160.50763255357742\n",
            "Epoch 40, Loss: 160.2940266430378\n",
            "Epoch 41, Loss: 161.45026433467865\n",
            "Epoch 42, Loss: 159.81784069538116\n",
            "Epoch 43, Loss: 159.1402763426304\n",
            "Epoch 44, Loss: 159.17669826745987\n",
            "Epoch 45, Loss: 158.81359073519707\n",
            "Epoch 46, Loss: 160.59090209007263\n",
            "Epoch 47, Loss: 159.01855051517487\n",
            "Epoch 48, Loss: 158.2425298690796\n",
            "Epoch 49, Loss: 158.74202239513397\n",
            "Epoch 50, Loss: 157.3388898074627\n",
            "Epoch 51, Loss: 156.97705814242363\n",
            "Epoch 52, Loss: 155.63221064209938\n",
            "Epoch 53, Loss: 156.38515174388885\n",
            "Epoch 54, Loss: 157.14675045013428\n",
            "Epoch 55, Loss: 153.85358002781868\n",
            "Epoch 56, Loss: 155.8259397149086\n",
            "Epoch 57, Loss: 154.77129119634628\n",
            "Epoch 58, Loss: 152.74306550621986\n",
            "Epoch 59, Loss: 155.3103386759758\n",
            "Epoch 60, Loss: 153.34896996617317\n",
            "Epoch 61, Loss: 156.49969962239265\n",
            "Epoch 62, Loss: 155.20312094688416\n",
            "Epoch 63, Loss: 154.0304865539074\n",
            "Epoch 64, Loss: 156.39278882741928\n",
            "Epoch 65, Loss: 158.11901080608368\n",
            "Epoch 66, Loss: 154.67697009444237\n",
            "Epoch 67, Loss: 153.04328483343124\n",
            "Epoch 68, Loss: 151.5869326889515\n",
            "Epoch 69, Loss: 153.30762946605682\n",
            "Epoch 70, Loss: 153.18042036890984\n",
            "Epoch 71, Loss: 152.2770410478115\n",
            "Epoch 72, Loss: 151.50640577077866\n",
            "Epoch 73, Loss: 152.65782144665718\n",
            "Epoch 74, Loss: 150.79057759046555\n",
            "Epoch 75, Loss: 151.6874595284462\n",
            "Epoch 76, Loss: 151.81440389156342\n",
            "Epoch 77, Loss: 151.51522415876389\n",
            "Epoch 78, Loss: 151.2345696091652\n",
            "Epoch 79, Loss: 149.77479103207588\n",
            "Epoch 80, Loss: 150.997162848711\n",
            "Epoch 81, Loss: 150.58101579546928\n",
            "Epoch 82, Loss: 150.62884762883186\n",
            "Epoch 83, Loss: 148.6187402009964\n",
            "Epoch 84, Loss: 149.86819553375244\n",
            "Epoch 85, Loss: 149.17209059000015\n",
            "Epoch 86, Loss: 147.06241518259048\n",
            "Epoch 87, Loss: 148.9765057861805\n",
            "Epoch 88, Loss: 147.83827084302902\n",
            "Epoch 89, Loss: 147.13263201713562\n",
            "Epoch 90, Loss: 147.97831410169601\n",
            "Epoch 91, Loss: 147.26197996735573\n",
            "Epoch 92, Loss: 149.30780091881752\n",
            "Epoch 93, Loss: 147.51249766349792\n",
            "Epoch 94, Loss: 147.39470714330673\n",
            "Epoch 95, Loss: 146.16089397668839\n",
            "Epoch 96, Loss: 146.79868119955063\n",
            "Epoch 97, Loss: 147.47339197993279\n",
            "Epoch 98, Loss: 145.1339422762394\n",
            "Epoch 99, Loss: 146.57638776302338\n",
            "Epoch 100, Loss: 144.94448859989643\n",
            "Epoch 101, Loss: 144.18280485272408\n",
            "Epoch 102, Loss: 145.91293516755104\n",
            "Epoch 103, Loss: 145.29726135730743\n",
            "Epoch 104, Loss: 146.76964285969734\n",
            "Epoch 105, Loss: 146.5502618700266\n",
            "Epoch 106, Loss: 145.805246591568\n",
            "Epoch 107, Loss: 143.9572439789772\n",
            "Epoch 108, Loss: 143.75932051241398\n",
            "Epoch 109, Loss: 146.92119681835175\n",
            "Epoch 110, Loss: 152.5740464925766\n",
            "Epoch 111, Loss: 147.29020428657532\n",
            "Epoch 112, Loss: 144.10809713602066\n",
            "Epoch 113, Loss: 144.33962479233742\n",
            "Epoch 114, Loss: 146.07177194952965\n",
            "Epoch 115, Loss: 144.1795269548893\n",
            "Epoch 116, Loss: 144.9374002814293\n",
            "Epoch 117, Loss: 145.48213677108288\n",
            "Epoch 118, Loss: 143.47821751236916\n",
            "Epoch 119, Loss: 142.13109654188156\n",
            "Epoch 120, Loss: 146.48564349114895\n",
            "Epoch 121, Loss: 141.89787340164185\n",
            "Epoch 122, Loss: 144.9510569423437\n",
            "Epoch 123, Loss: 142.91207069158554\n",
            "Epoch 124, Loss: 141.5402177721262\n",
            "Epoch 125, Loss: 142.9295068383217\n",
            "Epoch 126, Loss: 142.1195572912693\n",
            "Epoch 127, Loss: 142.4790686815977\n",
            "Epoch 128, Loss: 146.7982630431652\n",
            "Epoch 129, Loss: 145.87199646234512\n",
            "Epoch 130, Loss: 145.50209578871727\n",
            "Epoch 131, Loss: 142.20020692050457\n",
            "Epoch 132, Loss: 142.12638826668262\n",
            "Epoch 133, Loss: 143.12993422150612\n",
            "Epoch 134, Loss: 140.8395082950592\n",
            "Epoch 135, Loss: 143.5081741809845\n",
            "Epoch 136, Loss: 140.92124690115452\n",
            "Epoch 137, Loss: 140.64302957057953\n",
            "Epoch 138, Loss: 141.35760471224785\n",
            "Epoch 139, Loss: 140.43722528219223\n",
            "Epoch 140, Loss: 141.59087139368057\n",
            "Epoch 141, Loss: 140.45938113331795\n",
            "Epoch 142, Loss: 141.16157498955727\n",
            "Epoch 143, Loss: 139.7327255010605\n",
            "Epoch 144, Loss: 138.12994284927845\n",
            "Epoch 145, Loss: 140.33230958878994\n",
            "Epoch 146, Loss: 139.67475152015686\n",
            "Epoch 147, Loss: 138.9383145570755\n",
            "Epoch 148, Loss: 138.88202725350857\n",
            "Epoch 149, Loss: 140.02810701727867\n",
            "Epoch 150, Loss: 139.2108096331358\n",
            "Epoch 151, Loss: 137.52595791220665\n",
            "Epoch 152, Loss: 138.4372128546238\n",
            "Epoch 153, Loss: 140.47469641268253\n",
            "Epoch 154, Loss: 138.52778421342373\n",
            "Epoch 155, Loss: 139.135304749012\n",
            "Epoch 156, Loss: 137.26762627065182\n",
            "Epoch 157, Loss: 142.40097844600677\n",
            "Epoch 158, Loss: 137.77933430671692\n",
            "Epoch 159, Loss: 137.6792056262493\n",
            "Epoch 160, Loss: 139.37309819459915\n",
            "Epoch 161, Loss: 139.09847530722618\n",
            "Epoch 162, Loss: 136.27674263715744\n",
            "Epoch 163, Loss: 140.31731778383255\n",
            "Epoch 164, Loss: 137.03584234416485\n",
            "Epoch 165, Loss: 136.38571991026402\n",
            "Epoch 166, Loss: 138.06635493040085\n",
            "Epoch 167, Loss: 137.50211443006992\n",
            "Epoch 168, Loss: 137.30399519205093\n",
            "Epoch 169, Loss: 135.53759188950062\n",
            "Epoch 170, Loss: 135.59761156141758\n",
            "Epoch 171, Loss: 136.31275330483913\n",
            "Epoch 172, Loss: 137.53775410354137\n",
            "Epoch 173, Loss: 138.74115684628487\n",
            "Epoch 174, Loss: 137.79923462867737\n",
            "Epoch 175, Loss: 136.67580626904964\n",
            "Epoch 176, Loss: 136.11014844477177\n",
            "Epoch 177, Loss: 137.16580724716187\n",
            "Epoch 178, Loss: 136.7627464979887\n",
            "Epoch 179, Loss: 135.14862988889217\n",
            "Epoch 180, Loss: 134.71597830951214\n",
            "Epoch 181, Loss: 133.43155540525913\n",
            "Epoch 182, Loss: 134.68965592980385\n",
            "Epoch 183, Loss: 136.30555975437164\n",
            "Epoch 184, Loss: 133.44379250705242\n",
            "Epoch 185, Loss: 134.6769566088915\n",
            "Epoch 186, Loss: 135.2005688548088\n",
            "Epoch 187, Loss: 134.54582706093788\n",
            "Epoch 188, Loss: 135.3129217326641\n",
            "Epoch 189, Loss: 134.36954176425934\n",
            "Epoch 190, Loss: 134.3102084994316\n",
            "Epoch 191, Loss: 135.5448152422905\n",
            "Epoch 192, Loss: 137.31261762976646\n",
            "Epoch 193, Loss: 137.13067846000195\n",
            "Epoch 194, Loss: 134.0975526869297\n",
            "Epoch 195, Loss: 135.79350695014\n",
            "Epoch 196, Loss: 134.57997001707554\n",
            "Epoch 197, Loss: 133.55650193989277\n",
            "Epoch 198, Loss: 135.45337146520615\n",
            "Epoch 199, Loss: 133.85813987255096\n",
            "Epoch 200, Loss: 133.1156508922577\n",
            "Epoch 201, Loss: 134.19256435334682\n",
            "Epoch 202, Loss: 131.05811692774296\n",
            "Epoch 203, Loss: 134.47031663358212\n",
            "Epoch 204, Loss: 135.55632103979588\n",
            "Epoch 205, Loss: 133.7497861981392\n",
            "Epoch 206, Loss: 133.22773468494415\n",
            "Epoch 207, Loss: 132.30119562149048\n",
            "Epoch 208, Loss: 131.99439392983913\n",
            "Epoch 209, Loss: 133.58078163862228\n",
            "Epoch 210, Loss: 133.18827909231186\n",
            "Epoch 211, Loss: 132.83065788447857\n",
            "Epoch 212, Loss: 131.04605171084404\n",
            "Epoch 213, Loss: 133.70930343866348\n",
            "Epoch 214, Loss: 133.9150729328394\n",
            "Epoch 215, Loss: 130.3710011690855\n",
            "Epoch 216, Loss: 132.3209651261568\n",
            "Epoch 217, Loss: 131.45606988668442\n",
            "Epoch 218, Loss: 133.26508270204067\n",
            "Epoch 219, Loss: 131.48937395215034\n",
            "Epoch 220, Loss: 133.34798504412174\n",
            "Epoch 221, Loss: 128.25018945336342\n",
            "Epoch 222, Loss: 128.44389282166958\n",
            "Epoch 223, Loss: 134.60518483817577\n",
            "Epoch 224, Loss: 131.45731861889362\n",
            "Epoch 225, Loss: 129.5737630724907\n",
            "Epoch 226, Loss: 131.7658203691244\n",
            "Epoch 227, Loss: 129.93517319858074\n",
            "Epoch 228, Loss: 129.54951353371143\n",
            "Epoch 229, Loss: 128.526801943779\n",
            "Epoch 230, Loss: 128.19160540401936\n",
            "Epoch 231, Loss: 129.15383699536324\n",
            "Epoch 232, Loss: 129.3323110640049\n",
            "Epoch 233, Loss: 128.83139902353287\n",
            "Epoch 234, Loss: 129.15930096805096\n",
            "Epoch 235, Loss: 129.84799522161484\n",
            "Epoch 236, Loss: 131.04943439364433\n",
            "Epoch 237, Loss: 131.85952004790306\n",
            "Epoch 238, Loss: 130.80140027403831\n",
            "Epoch 239, Loss: 127.53765916824341\n",
            "Epoch 240, Loss: 130.33400776982307\n",
            "Epoch 241, Loss: 129.003382101655\n",
            "Epoch 242, Loss: 130.13074024021626\n",
            "Epoch 243, Loss: 128.81498397886753\n",
            "Epoch 244, Loss: 127.67888078093529\n",
            "Epoch 245, Loss: 127.62582987546921\n",
            "Epoch 246, Loss: 125.92067968845367\n",
            "Epoch 247, Loss: 131.59844028949738\n",
            "Epoch 248, Loss: 126.19067667424679\n",
            "Epoch 249, Loss: 128.01714430749416\n",
            "Epoch 250, Loss: 125.88622616231441\n",
            "Epoch 251, Loss: 128.6275508850813\n",
            "Epoch 252, Loss: 129.00430670380592\n",
            "Epoch 253, Loss: 125.78167867660522\n",
            "Epoch 254, Loss: 125.86651010811329\n",
            "Epoch 255, Loss: 137.8455676585436\n",
            "Epoch 256, Loss: 132.41933761537075\n",
            "Epoch 257, Loss: 130.15370497107506\n",
            "Epoch 258, Loss: 129.14294078946114\n",
            "Epoch 259, Loss: 128.9105180054903\n",
            "Epoch 260, Loss: 126.24539142847061\n",
            "Epoch 261, Loss: 125.39330971240997\n",
            "Epoch 262, Loss: 127.34285952150822\n",
            "Epoch 263, Loss: 127.46216477453709\n",
            "Epoch 264, Loss: 128.4427102804184\n",
            "Epoch 265, Loss: 126.32831755280495\n",
            "Epoch 266, Loss: 127.45204231142998\n",
            "Epoch 267, Loss: 127.8531413525343\n",
            "Epoch 268, Loss: 127.22283798456192\n",
            "Epoch 269, Loss: 127.00343005359173\n",
            "Epoch 270, Loss: 128.08471110463142\n",
            "Epoch 271, Loss: 125.53644175082445\n",
            "Epoch 272, Loss: 127.6879123300314\n",
            "Epoch 273, Loss: 123.75871360301971\n",
            "Epoch 274, Loss: 125.28650949895382\n",
            "Epoch 275, Loss: 128.00104351341724\n",
            "Epoch 276, Loss: 125.38535588979721\n",
            "Epoch 277, Loss: 124.11767941713333\n",
            "Epoch 278, Loss: 124.25868276506662\n",
            "Epoch 279, Loss: 127.7249058932066\n",
            "Epoch 280, Loss: 126.50441646575928\n",
            "Epoch 281, Loss: 128.27292902767658\n",
            "Epoch 282, Loss: 127.92481337487698\n",
            "Epoch 283, Loss: 125.97761063277721\n",
            "Epoch 284, Loss: 125.2618595212698\n",
            "Epoch 285, Loss: 124.18604062497616\n",
            "Epoch 286, Loss: 127.97138419747353\n",
            "Epoch 287, Loss: 126.89105877280235\n",
            "Epoch 288, Loss: 123.13733619451523\n",
            "Epoch 289, Loss: 122.77539370954037\n",
            "Epoch 290, Loss: 124.89536318182945\n",
            "Epoch 291, Loss: 126.29034093022346\n",
            "Epoch 292, Loss: 124.38135704398155\n",
            "Epoch 293, Loss: 124.97612200677395\n",
            "Epoch 294, Loss: 124.12508647143841\n",
            "Epoch 295, Loss: 121.1082395017147\n",
            "Epoch 296, Loss: 126.27210834622383\n",
            "Epoch 297, Loss: 125.46407189965248\n",
            "Epoch 298, Loss: 124.84619376063347\n",
            "Epoch 299, Loss: 126.72253267467022\n",
            "Epoch 300, Loss: 123.48246462643147\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(300):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for data in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "\n",
        "        # Forward pass\n",
        "        #node_output = model(data.x, data.edge_index, data.edge_attr)\n",
        "\n",
        "        # Aggregate node outputs to graph-level predictions using batch information\n",
        "        #graph_output = global_mean_pool(node_output, data.batch)\n",
        "\n",
        "        # Compute loss\n",
        "        #loss = F.nll_loss(graph_output, data.y)\n",
        "        #loss = F.nll_loss(out, data.y)\n",
        "        loss = F.cross_entropy(out, data.y)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RRnsGwNlxl9c",
      "metadata": {
        "id": "RRnsGwNlxl9c"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"gat_model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Z56KSzZkpLdF",
      "metadata": {
        "id": "Z56KSzZkpLdF"
      },
      "source": [
        "For each graph in the test set:\n",
        "- Forward Pass: Generate node-level embeddings using the trained model.\n",
        "- Graph-Level Aggregation: Aggregate node-level outputs to a single graph-level prediction using `global_mean_pool`.\n",
        "- Class Prediction: Use the `argmax` function to get the predicted class for the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "aUesVkvq1SDL",
      "metadata": {
        "id": "aUesVkvq1SDL"
      },
      "outputs": [],
      "source": [
        "# Evaluate model on test set (with graph-level pooling)\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "all_probs = []\n",
        "\n",
        "\n",
        "for data in test_graphs:\n",
        "\n",
        "    # Forward pass (note: model already pools and gives graph-level prediction)\n",
        "    out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "\n",
        "    # Get predicted class\n",
        "    preds = out.argmax(dim=1)\n",
        "\n",
        "    # Forward pass\n",
        "    #node_output = model(data.x, data.edge_index, data.edge_attr)\n",
        "\n",
        "    # Aggregate node outputs to graph-level predictions using batch information\n",
        "    #graph_prediction = global_mean_pool(node_output, data.batch)\n",
        "\n",
        "    # Get predicted class\n",
        "    #preds = graph_prediction.argmax(dim=1)\n",
        "\n",
        "    # Store predicted probabilities for the positive class (class 1)\n",
        "    probs = torch.softmax(out, dim=1)[:, 1].detach().numpy()  # Probabilities for class 1\n",
        "\n",
        "    all_preds.append(preds.item())\n",
        "    all_labels.append(data.y.item())\n",
        "    all_probs.extend(probs)  # Append probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "rg5NZ7ymWgYL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rg5NZ7ymWgYL",
        "outputId": "ed249721-a23c-44ae-ba74-7ab47bc394b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Control       0.70      0.56      0.62       477\n",
            "    Epilepsy       0.71      0.82      0.76       625\n",
            "\n",
            "    accuracy                           0.71      1102\n",
            "   macro avg       0.71      0.69      0.69      1102\n",
            "weighted avg       0.71      0.71      0.70      1102\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Generate classification report\n",
        "print(classification_report(all_labels, all_preds, target_names=['Control', 'Epilepsy']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "7cFuNsmSCZFr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "7cFuNsmSCZFr",
        "outputId": "d82d47dd-369d-493a-9f58-64be51f1e7ca"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 0.76\n"
          ]
        }
      ],
      "source": [
        "# Convert lists to numpy arrays\n",
        "all_labels = np.array(all_labels)\n",
        "all_probs = np.array(all_probs)\n",
        "\n",
        "#from sklearn.metrics import accuracy_score\n",
        "#threshold = 0.5\n",
        "#all_probs = (all_probs >= threshold).astype(int)\n",
        "#accuracy = accuracy_score(all_labels, all_probs)\n",
        "#print(accuracy)\n",
        "\n",
        "# Calculate ROC curve and AUC\n",
        "fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Alternatively, calculate ROC AUC score directly\n",
        "roc_auc_score_value = roc_auc_score(all_labels, all_probs)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', label='Random Guess')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR)')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "# Print AUC value\n",
        "print(f\"AUC: {roc_auc:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VxB6UJYVvDUi",
      "metadata": {
        "id": "VxB6UJYVvDUi"
      },
      "source": [
        "The results show that the model is doing a decent job overall, with an accuracy of 65% and an AUC of 0.65, which is better than random guessing but still has room for improvement. It performs well at identifying instances of Epilepsy, correctly predicting 92% of them, but struggles with the Control class, only catching 28% of those cases. This suggests the model is better at recognizing Epilepsy but misses many Control instances, which could have been caused by data quality and lack of consistency in some samples. Model might have been improved with stricter rules for the quality of data, however there were not too many samples to begin with so we decided against it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "veDmqEeIXnSl",
      "metadata": {
        "id": "veDmqEeIXnSl"
      },
      "source": [
        "<!-- **Ways to possibly improve Model Performance?**\n",
        "\n",
        " 1. Add More Node Features\n",
        "Currently, you are using Katz FD and energy in 4 frequency bands (delta, theta, alpha, beta). You can include additional features like:\n",
        "- **Signal Entropy:** A measure of the complexity of the EEG signal.\n",
        "- **Inter-channel Correlation:** Measures the similarity between signals from different nodes.\n",
        "- **Amplitude Variability:** Distribution of amplitude values over time.\n",
        "\n",
        "---\n",
        "\n",
        " 2. Experiment with Model Hyperparameters\n",
        "Try modifying the following parameters:\n",
        "- **Increase `hidden_channels`:** Add more hidden channels in the GAT model to capture complex patterns.\n",
        "- **Increase the number of `heads` in GATConv:** More attention heads allow the model to learn richer relationships between nodes.\n",
        "\n",
        "---\n",
        "\n",
        "3. Regularization\n",
        "To prevent overfitting, apply regularization techniques:\n",
        "- **Dropout:** Add dropout layers to the GAT model to randomly drop some neurons during training.\n",
        "- **L2 Regularization:** Penalize large weights in the model to enforce simplicity.\n",
        "\n",
        "---\n",
        "\n",
        "4. Balance Class Weights\n",
        "If the dataset is imbalanced, use class weights to give more importance to underrepresented classes.\n",
        "\n",
        "---\n",
        "\n",
        "5. Increase the Number of Epochs\n",
        "Train the model for more epochs to allow it to learn better patterns in the data. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90kJIUBaame7",
      "metadata": {
        "id": "90kJIUBaame7"
      },
      "source": [
        "### Interpretability"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Q9PGTzWopxZB",
      "metadata": {
        "id": "Q9PGTzWopxZB"
      },
      "source": [
        "This process analyzes the importance of graph edges based on the attention weights learned by the GAT model. Attention weights indicate how much importance the model assigns to connections between nodes (edges) during message passing.\n",
        "\n",
        "For each of the first 5 graphs in the test set:\n",
        "\n",
        "1. Retrieve Attention Weights: Use the get_attention_weights function to extract the weights.\n",
        "2. Convert to NumPy: Convert the weights to a NumPy array for visualization.\n",
        "3. Plot Histogram: Display the distribution of attention weights using a histogram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4FfiiTeb1se",
      "metadata": {
        "id": "b4FfiiTeb1se"
      },
      "outputs": [],
      "source": [
        "def get_attention_weights(model, data):\n",
        "    with torch.no_grad():\n",
        "        # Perform forward pass and retrieve attention weights\n",
        "        _, attention_weights = model.gat1(data.x, data.edge_index, return_attention_weights=True)\n",
        "    return attention_weights\n",
        "\n",
        "# Iterate through multiple graphs and visualize attention weights\n",
        "for idx, data in enumerate(test_graphs[:5]):  # Analyze first 5 graphs\n",
        "    attention_weights = get_attention_weights(model, data)\n",
        "\n",
        "    # Convert attention weights to NumPy\n",
        "    attention_weights = attention_weights[1].numpy()  # Use [1] to get the actual attention weights\n",
        "\n",
        "    # Flatten the attention weights for histogram\n",
        "    flattened_weights = attention_weights.flatten()\n",
        "\n",
        "    # Plot histogram\n",
        "    plt.hist(flattened_weights, bins=20, color='blue', alpha=0.7)\n",
        "    plt.xlabel(\"Attention Weights\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.title(f\"Attention Weights for Graph {idx + 1}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "esWLljXGwXH8",
      "metadata": {
        "id": "esWLljXGwXH8"
      },
      "source": [
        "From the attention weights plot we see that the attention weights are predominantly weighting most edges as not important at all, some as very important, and much less values in between 0 and 1, which suggests the model is not effectively learning to differentiate between edges."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NQOKtvEmgkKp",
      "metadata": {
        "id": "NQOKtvEmgkKp"
      },
      "source": [
        "Now using Gat for node importance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qrYFitVcgOmP",
      "metadata": {
        "id": "qrYFitVcgOmP"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "data = test_graphs[0]  # Select one graph for analysis\n",
        "batch = torch.zeros(data.x.size(0), dtype=torch.long)  # All nodes belong to the same graph\n",
        "\n",
        "# Create Grad-CAM instance\n",
        "grad_cam = GradCamGAT(model, target_layer=model.gat1)  # Use the first GAT layer as the target\n",
        "\n",
        "# Forward pass through Grad-CAM\n",
        "graph_output, node_output = grad_cam.forward(data.x, data.edge_index, data.edge_attr, batch)\n",
        "\n",
        "# Backward pass to compute gradients\n",
        "predicted_class = graph_output.argmax(dim=1).item()  # Get the predicted class\n",
        "grad_cam.backward(graph_output, class_idx=predicted_class)\n",
        "\n",
        "# Compute node importance\n",
        "node_importance = grad_cam.get_node_importance()\n",
        "\n",
        "# Visualize node importance\n",
        "plt.bar(range(len(node_importance)), node_importance.numpy(), color='green')\n",
        "plt.xlabel(\"Node Index\")\n",
        "plt.ylabel(\"Importance\")\n",
        "plt.title(\"Node Importance via Grad-CAM\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lYdMqzUK9_gf",
      "metadata": {
        "id": "lYdMqzUK9_gf"
      },
      "source": [
        "One of the nodes has a significantly higher importance than the others, one is deemed insignificant (<0). Concerning thing in above results is that the rest are uniform which signals that model is likely having underfitting issues."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uLL7HjJD3Z7O",
      "metadata": {
        "id": "uLL7HjJD3Z7O"
      },
      "source": [
        "# Guinea-Bissau Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caLOrMVqzNen",
      "metadata": {
        "id": "caLOrMVqzNen"
      },
      "source": [
        "We'll do the same for Guinea-Bissau dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "T6pv8-k28mDf",
      "metadata": {
        "id": "T6pv8-k28mDf"
      },
      "source": [
        "Process all EEG files, create graphs and feature files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rMfF0oV98mDg",
      "metadata": {
        "id": "rMfF0oV98mDg"
      },
      "outputs": [],
      "source": [
        "#process_all_files(gb_df, \"output4/graphs_gb\", \"output4/features_gb\")\n",
        "\n",
        "n_epochs_per_sample = process_all_files_with_epochs(\n",
        "    ni_df,\n",
        "    \"output4/graphs_gb_epoched\",\n",
        "    \"output4/features_gb_epoched\",\n",
        "    duration=5,\n",
        "    overlap=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NCfej49q8mDg",
      "metadata": {
        "id": "NCfej49q8mDg"
      },
      "source": [
        "Similarly, let's load one sample graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nmAbu9xu8mDg",
      "metadata": {
        "id": "nmAbu9xu8mDg"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "\n",
        "# Path to a sample .gml file\n",
        "gml_file = \"output4/graphs_gb/GB-6_graph.gml\"\n",
        "\n",
        "# Load the graph from the .gml file\n",
        "graph = nx.read_gml(gml_file)\n",
        "\n",
        "# Print basic information about the graph\n",
        "print(\"Number of nodes:\", graph.number_of_nodes())\n",
        "print(\"Number of edges:\", graph.number_of_edges())\n",
        "\n",
        "# Display a few edges with their weights\n",
        "print(\"Sample edges with weights:\")\n",
        "for u, v, weight in graph.edges(data='weight'):\n",
        "    print(f\"({u}, {v}) -> weight: {weight}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BYjvn-ii8mDg",
      "metadata": {
        "id": "BYjvn-ii8mDg"
      },
      "source": [
        "Visualizing the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "etyvWoSU8mDg",
      "metadata": {
        "id": "etyvWoSU8mDg"
      },
      "outputs": [],
      "source": [
        "# Load the graph\n",
        "graph = nx.read_gml(gml_file)\n",
        "\n",
        "# Draw the graph\n",
        "plt.figure(figsize=(10, 8))\n",
        "pos = nx.spring_layout(graph)  # Automatic layout for nodes\n",
        "nx.draw(\n",
        "    graph,\n",
        "    pos,\n",
        "    with_labels=True,\n",
        "    node_size=700,\n",
        "    node_color=\"lightblue\",\n",
        "    font_size=10,\n",
        "    font_color=\"black\",\n",
        ")\n",
        "# Add edge labels showing weights\n",
        "nx.draw_networkx_edge_labels(\n",
        "    graph, pos, edge_labels={(u, v): f\"{d['weight']:.2f}\" for u, v, d in graph.edges(data=True)}\n",
        ")\n",
        "plt.title(\"Visualization of EEG Graph (PLV)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H1qELjia8mDg",
      "metadata": {
        "id": "H1qELjia8mDg"
      },
      "source": [
        "### GAT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NNfkP1rF8mDg",
      "metadata": {
        "id": "NNfkP1rF8mDg"
      },
      "outputs": [],
      "source": [
        "# Directory containing .gml files\n",
        "gml_dir = \"output4/graphs_gb\"\n",
        "feature_dir = \"output4/features_gb\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jRiWzlYp8mDh",
      "metadata": {
        "id": "jRiWzlYp8mDh"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "#graphs, labels = load_graphs_and_labels_with_features(gml_dir, feature_dir, gb_df)\n",
        "\n",
        "# Load the epoched data with properly expanded labels\n",
        "graphs, labels = load_graphs_and_labels_with_epochs(\n",
        "    \"output4/graphs_gb_epoched\",\n",
        "    \"output4/features_gb_epoched\",\n",
        "    ni_df,\n",
        "    n_epochs_per_sample\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vYeCn48m8mDh",
      "metadata": {
        "id": "vYeCn48m8mDh"
      },
      "source": [
        "To ensure that both classes (Control and Epilepsy) are proportionally represented in the train and test datasets, we use a stratified split. This ensures the class balance is maintained in both subsets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uFhEkoB_8mDh",
      "metadata": {
        "id": "uFhEkoB_8mDh"
      },
      "outputs": [],
      "source": [
        "# Stratified split to ensure both classes are represented in train and test sets\n",
        "train_graphs, test_graphs, train_labels, test_labels = train_test_split(\n",
        "    graphs, labels, test_size=0.2, stratify=labels, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Train class distribution:\", Counter(train_labels.numpy()))\n",
        "print(\"Test class distribution:\", Counter(test_labels.numpy()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D8W_ewNH8mDh",
      "metadata": {
        "id": "D8W_ewNH8mDh"
      },
      "source": [
        "Setting up the DataLoader and Model Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a-j7lj1d8mDh",
      "metadata": {
        "id": "a-j7lj1d8mDh"
      },
      "outputs": [],
      "source": [
        "# Create DataLoader\n",
        "train_loader = DataLoader(train_graphs, batch_size=16, shuffle=True)\n",
        "\n",
        "# Initialize model\n",
        "model = GATModel(in_channels=5, hidden_channels=8, out_channels=2)  # Binary classification\n",
        "#optimizer = Adam(model.parameters(), lr=0.01)\n",
        "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=5e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NuHNAtbL8mDi",
      "metadata": {
        "id": "NuHNAtbL8mDi"
      },
      "outputs": [],
      "source": [
        "for epoch in range(50):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        node_output = model(data.x, data.edge_index, data.edge_attr)\n",
        "\n",
        "        # Aggregate node outputs to graph-level predictions using batch information\n",
        "        graph_output = global_mean_pool(node_output, data.batch)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = F.nll_loss(graph_output, data.y)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ToF9qCyG8mDi",
      "metadata": {
        "id": "ToF9qCyG8mDi"
      },
      "outputs": [],
      "source": [
        "# Evaluate model on test set (with graph-level pooling)\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "all_probs = []\n",
        "\n",
        "for data in test_graphs:\n",
        "    # Forward pass\n",
        "    node_output = model(data.x, data.edge_index, data.edge_attr)\n",
        "\n",
        "    # Aggregate node outputs to graph-level predictions using batch information\n",
        "    graph_prediction = global_mean_pool(node_output, data.batch)\n",
        "\n",
        "    # Get predicted class\n",
        "    preds = graph_prediction.argmax(dim=1)\n",
        "\n",
        "\n",
        "    # Store predicted probabilities for the positive class (class 1)\n",
        "    probs = torch.softmax(graph_prediction, dim=1)[:, 1].detach().numpy()  # Probabilities for class 1\n",
        "\n",
        "\n",
        "    all_preds.append(preds.item())\n",
        "    all_labels.append(data.y.item())\n",
        "\n",
        "    all_probs.extend(probs)  # Append probabilities\n",
        "\n",
        "# Generate classification report\n",
        "print(classification_report(all_labels, all_preds, target_names=['Control', 'Epilepsy']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-a-IUxJY5UHh",
      "metadata": {
        "id": "-a-IUxJY5UHh"
      },
      "source": [
        "The model is struggling to correctly identify instances of the **Control** class, catching only 11% of them, while it performs much better with the **Epilepsy** class, correctly identifying 91% of those cases. Overall, the model's predictions are about 55% accurate, but the low recall for the Control class indicates it misses many of those instances. This suggests the model is biased toward the Epilepsy class and could benefit from more balanced dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wBRfzFOaGtrj",
      "metadata": {
        "id": "wBRfzFOaGtrj"
      },
      "outputs": [],
      "source": [
        "# Convert lists to numpy arrays\n",
        "all_labels = np.array(all_labels)\n",
        "all_probs = np.array(all_probs)\n",
        "\n",
        "# Calculate ROC curve and AUC\n",
        "fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Alternatively, calculate ROC AUC score directly\n",
        "roc_auc_score_value = roc_auc_score(all_labels, all_probs)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', label='Random Guess')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR)')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "# Print AUC value\n",
        "print(f\"AUC: {roc_auc:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I-oFZ4HQ8mDi",
      "metadata": {
        "id": "I-oFZ4HQ8mDi"
      },
      "source": [
        "### Interpretability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "We6P4jix8mDi",
      "metadata": {
        "id": "We6P4jix8mDi"
      },
      "outputs": [],
      "source": [
        "# Extract attention weights\n",
        "def get_attention_weights(model, data):\n",
        "    with torch.no_grad():\n",
        "        # Perform forward pass and retrieve attention weights\n",
        "        _, attention_weights = model.gat1(data.x, data.edge_index, return_attention_weights=True)\n",
        "    return attention_weights\n",
        "\n",
        "# Iterate through multiple graphs and visualize attention weights\n",
        "for idx, data in enumerate(test_graphs[:7]):  # Analyze first 5 graphs\n",
        "    attention_weights = get_attention_weights(model, data)\n",
        "\n",
        "    # Convert attention weights to NumPy\n",
        "    attention_weights = attention_weights[1].numpy()  # Use [1] to get the actual attention weights\n",
        "\n",
        "    # Flatten the attention weights for histogram\n",
        "    flattened_weights = attention_weights.flatten()\n",
        "\n",
        "    # Plot histogram\n",
        "    plt.hist(flattened_weights, bins=20, color='blue', alpha=0.7)\n",
        "    plt.xlabel(\"Attention Weights\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.title(f\"Attention Weights for Graph {idx + 1}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6AE7TUy3gVQ",
      "metadata": {
        "id": "d6AE7TUy3gVQ"
      },
      "source": [
        "Again we mostly see that model considers majory of node unimportany, around 20 are very important, and only some are somewhere in-between. Once again we observe this diffculty with learning these patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0YUEVh58mDj",
      "metadata": {
        "id": "d0YUEVh58mDj"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "data = test_graphs[0]  # Select one graph for analysis\n",
        "batch = torch.zeros(data.x.size(0), dtype=torch.long)  # All nodes belong to the same graph\n",
        "\n",
        "# Create Grad-CAM instance\n",
        "grad_cam = GradCamGAT(model, target_layer=model.gat1)  # Use the first GAT layer as the target\n",
        "\n",
        "# Forward pass through Grad-CAM\n",
        "graph_output, node_output = grad_cam.forward(data.x, data.edge_index, data.edge_attr, batch)\n",
        "\n",
        "# Backward pass to compute gradients\n",
        "predicted_class = graph_output.argmax(dim=1).item()  # Get the predicted class\n",
        "grad_cam.backward(graph_output, class_idx=predicted_class)\n",
        "\n",
        "# Compute node importance\n",
        "node_importance = grad_cam.get_node_importance()\n",
        "\n",
        "# Visualize node importance\n",
        "plt.bar(range(len(node_importance)), node_importance.numpy(), color='green')\n",
        "plt.xlabel(\"Node Index\")\n",
        "plt.ylabel(\"Importance\")\n",
        "plt.title(\"Node Importance via Grad-CAM\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rbrceY7E3z99",
      "metadata": {
        "id": "rbrceY7E3z99"
      },
      "source": [
        "Higher importance scores indicate that the model considers those nodes more influential in making its predictions. We see that the importance scores are  uniform in some groups, it implies that the model isn't effectively distinguishing between important and less important nodes in certain groups of nodes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VATVXG4H9GZ-",
      "metadata": {
        "id": "VATVXG4H9GZ-"
      },
      "source": [
        "# Final observations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4yRbnf2N9LLK",
      "metadata": {
        "id": "4yRbnf2N9LLK"
      },
      "source": [
        "From above results we see that there is room for improvement, starting from more restrictive data cleaning/selection as it presented some difficulties such as corrupted samples or inconsitencies in the size of samples etc. This comes with a risk as the dataset is not large and samples come from cheap EEG measuring devices. We also observe that ther could be a chance for improvement with for example better parameter choices for the models as we have seen some underfitting tendencies and difficulties with pattern recognition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_NIIyunG37SP",
      "metadata": {
        "id": "_NIIyunG37SP"
      },
      "outputs": [],
      "source": [
        "#Alex Sandbox from here\n",
        "#!pip install nilearn\n",
        "'''\n",
        "!pip install mne pyvista pyvistaqt qtpy\n",
        "import mne\n",
        "import numpy as np\n",
        "import pyvista as pv\n",
        "\n",
        "# Load FreeSurfer brain mesh\n",
        "brain = mne.viz.Brain('fsaverage', subjects_dir='/path/to/freesurfer/subjects')\n",
        "\n",
        "# Example EEG electrode positions in MNI space\n",
        "coords = np.array([\n",
        "    [-30, 40, 50], [30, 40, 50], [-50, 10, 40], [50, 10, 40],\n",
        "    [-40, -20, 30], [40, -20, 30], [-10, -50, 20], [10, -50, 20]\n",
        "])\n",
        "\n",
        "# Create connectivity matrix\n",
        "connectivity = np.random.rand(len(coords), len(coords))\n",
        "\n",
        "# Create 3D PyVista plot\n",
        "plotter = pv.Plotter()\n",
        "plotter.add_mesh(brain._brain_mesh, color=\"white\", opacity=0.3)\n",
        "\n",
        "# Add EEG sensors as spheres\n",
        "for coord in coords:\n",
        "    plotter.add_mesh(pv.Sphere(radius=2, center=coord), color=\"red\")\n",
        "\n",
        "# Add connectivity edges\n",
        "for i in range(len(coords)):\n",
        "    for j in range(i+1, len(coords)):\n",
        "        if connectivity[i, j] > 0.5:  # Thresholding\n",
        "            plotter.add_mesh(pv.Line(coords[i], coords[j]), color=\"blue\", line_width=2)\n",
        "\n",
        "plotter.show()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oPsLI12dqYEq",
      "metadata": {
        "id": "oPsLI12dqYEq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "y5UA83fAOHLd"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}